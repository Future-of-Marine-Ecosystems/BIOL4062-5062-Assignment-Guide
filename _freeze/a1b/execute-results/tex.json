{
  "hash": "6c68ff09a130c0305831dbce18f01e02",
  "result": {
    "engine": "knitr",
    "markdown": "# **Assignment 1b:** Linear Discriminant Analysis\n\nAssignment 1b focuses on Linear Discriminant Analysis (LDA), also known as Canonical Variate Analysis. LDA is used to disclose relationships between groups, create models to differentiate between groups based on data, and discern the contribution of different variables to a model's ability do discriminate between groups.\n\nFor this tutorial, we'll be using `snake.csv`.\n\n## Looking at the data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load in data\nsnake = read.csv('snake.csv')\n\n# Look at data\nhead(snake)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Species   M1   M2   M3   M4   M5   M6\n1     A   41.6  6.7  8.2 12.2 24.7 27.0\n2     A   40.2  8.5  9.2 15.5 27.1 30.3\n3     A   40.4 12.6 14.2 19.6 46.9 26.8\n4     A   26.4  9.0  8.6 14.0 37.6 32.2\n5     A   34.4  7.0 12.1 11.1 31.0 35.8\n6     A   38.8  8.2 10.2 12.4 42.2 33.6\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(snake)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35  7\n```\n\n\n:::\n:::\n\n\n\n\n\n\nOur data is a 35 row, 7 column data frame. The first column identifies the species of snake (A or B). The other columns are morphological measurements of each individual snake. We want to know if we can use the morphological measurements of the snakes to determine their species. Let's keep examining the data:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a boxplot\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert the data to long format so we can use ggplot\nsnake_long = pivot_longer(snake, # Enter data\n                          colnames(snake)[-1], # Pivot all columns except species\n                          names_to = 'Measurement', values_to = 'Value') # Feed labels to new data frame\n\n# Lets take a look at the new data frame\nhead(snake_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 3\n  Species  Measurement Value\n  <chr>    <chr>       <dbl>\n1 \"   A  \" M1           41.6\n2 \"   A  \" M2            6.7\n3 \"   A  \" M3            8.2\n4 \"   A  \" M4           12.2\n5 \"   A  \" M5           24.7\n6 \"   A  \" M6           27  \n```\n\n\n:::\n\n```{.r .cell-code}\n# We've converted from wide format to long format,\n# now all the data values are contained in a single column\n# which is described by a metadata column\n\n# You can also do this with melt from reshape2\nlibrary(reshape2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(melt(snake))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing Species as id variables\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Species variable value\n1     A         M1  41.6\n2     A         M1  40.2\n3     A         M1  40.4\n4     A         M1  26.4\n5     A         M1  34.4\n6     A         M1  38.8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Let's make a boxplot\nggplot(snake_long, aes(x = Measurement, y = Value, fill = Species)) +\n  geom_boxplot() + theme_classic()\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# We can do this in R base plot too\nboxplot(Value ~ Species*Measurement, # Plot value by species and measurement\n        data = snake_long, col = c('coral', 'turquoise2'), # Color by species\n        xaxt = 'n', xlab = 'Measurement') # Remove and label x axis\nlegend('topleft', legend = c('Species A', 'Species B'), fill = c('coral', 'turquoise2')) # Add a legend\naxis(1, at = seq(1.5,11.5,2), labels = colnames(snake)[-1]) # Add x axis back in with appropriate labels\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-2-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nSome of our measurements are very similar across species, and others are quite different. Do they differ statistically as a whole?\n\n## MANOVA\n\nThe purpose of LDA is to try to discriminate our snakes into species based on their measurements. However, that only makes sense to do if our two species of snake actually differ across the measurements. Our first step then is to discern whether our snake species differ as a multivariate whole. We'll do this using a MANOVA.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run MANOVA\nsm = manova(cbind(M1,M2,M3,M4,M5,M6) ~ Species, data = snake)\nsummary(sm, test = 'Hotelling')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Df Hotelling-Lawley approx F num Df den Df   Pr(>F)    \nSpecies    1           1.2263   5.7229      6     28 0.000552 ***\nResiduals 33                                                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(sm, test = 'Wilks')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Df   Wilks approx F num Df den Df   Pr(>F)    \nSpecies    1 0.44917   5.7229      6     28 0.000552 ***\nResiduals 33                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\nBy both the Hotelling's and Wilks' tests, our MANOVA is significant, indicating the snake species vary as a multivariate whole.\n\nWhat about our assumptions though? Our MANOVA assumptions are independence, normality, linearity, and homogeneity of covariances. You've been told to assume the latter, so let's skip that one. Independence states that measurements of each snake are independent from all others. For example, it would be violated if our data were related to each other - for example, if some of our snakes were closely related, or if the graduate students measuring them were using different methods. We don't have information about how this data was collected, so we cannot assess independence. We'll skip that one as well.\n\nLet's start by testing for normality:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Testing normality\nlibrary(mvnormtest)\nmshapiro.test(t(sm$residuals))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  Z\nW = 0.91571, p-value = 0.01075\n```\n\n\n:::\n:::\n\n\n\n\n\n\nUh oh, the residuals are significantly non-normal. Let's take a look at them visually:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual histogram\nhist(t(sm$residuals), breaks = 20)\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nVisually, our residuals actually look quite close to normal. There may be some slight skew, or outliers that are forcing our residuals to statistical non-normality. We might be able to fix this by removing multivariate outliers, or by transforming some of our data (feel free to play around with these ideas!), but based on the shape of our residuals, it is unlikely that our model is fatally biased, and we may end up doing more harm than good. Based on this, we can conclude that our two species have significantly different morphometries given the measurements provided.\n\n## Linear Discriminant Analysis\n\nNow that we've confirmed our species differ as a multivariate whole, we can try to use LDA to build a model to predict which species each snake belongs to based on its measurements.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# LDA\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code}\nldaf1 <- lda(Species ~ M1+M2+M3+M4+M5+M6, snake)\nldaf1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(Species ~ M1 + M2 + M3 + M4 + M5 + M6, data = snake)\n\nPrior probabilities of groups:\n      A         B   \n0.2857143 0.7142857 \n\nGroup means:\n           M1     M2    M3    M4     M5     M6\n   A   32.700  9.410 10.16 15.54 35.290 28.950\n   B   31.496 12.128 10.18 16.78 47.356 21.752\n\nCoefficients of linear discriminants:\n           LD1\nM1  0.01428023\nM2  0.29104494\nM3 -0.07327616\nM4 -0.05544769\nM5  0.03629586\nM6 -0.17208517\n```\n\n\n:::\n:::\n\n\n\n\n\n\nRunning our LDA object tells us the prior probabilities used for each species (the proportion of each species in the data), the group means for each measure on each species, and the linear discriminant (LD1) for each measure. We can then plot the LD1 value for each individual:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot discriminant function analysis\n\n# Create a data frame to plot\nldaf_plot = cbind(snake, # Data\n                  predict(ldaf1)$x, # LD1 value for each individual given its measurements\n                  index = seq(1,nrow(snake), 1)) # Row/Individual number\n\n# Plot\nplot(LD1 ~ index, data = ldaf_plot, col = as.factor(snake$Species), pch = 16)\nlegend('topleft', legend = c('A', 'B'), col = c(1, 2), pch = 16) # Add legend\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nHere we can see higher LD1 values are associated with species B, while lower LD1 values are associated with species A. This is just based on model fit however; how do we know we aren't overfitting? One way to avoid overfitting is by jackknifing (AKA leave-one-out cross validation in this context). This method runs the model once without each point in the dataset, then calculates the posterior probability that the left out point belongs to each species. Let's try it out:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# LDA 2, CV = T\nldaf2 = lda(Species ~ M1+M2+M3+M4+M5+M6, snake, CV = T)\n\n# Gather posteriors\nas.data.frame(cbind(ldaf2$posterior, # Pull posteriors from ldaf2\n                    ResultantSpp=as.character(ldaf2$class))) # Pull predicted species (i.e. species with the higher posterior probability)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    A                    B   ResultantSpp\n1     0.897801237948675    0.102198762051325          A  \n2     0.957033498274347   0.0429665017256533          A  \n3   0.00486396795570835    0.995136032044292          B  \n4     0.939579607872302   0.0604203921276982          A  \n5     0.999020574119129 0.000979425880871105          A  \n6     0.958283942083953   0.0417160579160474          A  \n7     0.859914694048174    0.140085305951826          A  \n8     0.079027668947901    0.920972331052099          B  \n9     0.250711809994117    0.749288190005883          B  \n10    0.277233534989758    0.722766465010243          B  \n11   0.0654339037846645    0.934566096215336          B  \n12 8.13045175684641e-05    0.999918695482432          B  \n13  0.00857331675606217    0.991426683243938          B  \n14    0.119793120831736    0.880206879168264          B  \n15    0.868897347918874    0.131102652081126          A  \n16    0.291404395123414    0.708595604876586          B  \n17    0.580893601645515    0.419106398354485          A  \n18    0.407526292222816    0.592473707777184          B  \n19   0.0971393472407664    0.902860652759234          B  \n20   0.0629455676122023    0.937054432387798          B  \n21   0.0262176442553782    0.973782355744622          B  \n22   0.0110464412594654    0.988953558740534          B  \n23    0.379676706769168    0.620323293230832          B  \n24  0.00300786068222621    0.996992139317774          B  \n25   0.0331152011340242    0.966884798865976          B  \n26  0.00270158005931189    0.997298419940688          B  \n27   0.0161164609849136    0.983883539015086          B  \n28  0.00346528534198867    0.996534714658011          B  \n29    0.761253716426844    0.238746283573156          A  \n30   0.0597294571669353    0.940270542833065          B  \n31  0.00139900114299065    0.998600998857009          B  \n32    0.014630451548708    0.985369548451292          B  \n33   0.0215114427320868    0.978488557267913          B  \n34  0.00359029891803416    0.996409701081966          B  \n35 8.92739861715449e-05    0.999910726013828          B  \n```\n\n\n:::\n:::\n\n\n\n\n\n\nHow does this differ from the predictions from our first model?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pull ldaf1 model predictions\nldaf_pred = predict(ldaf1)$class\n\n# Gather Predictions\nldaf_diff = data.frame(ldaf1 = as.character(ldaf_pred), ldaf2 = as.character(ldaf2$class))\n\n# Add match column\nldaf_diff$match = (ldaf_diff$ldaf1 == ldaf_diff$ldaf2)\n\n# Which ones are different?\nldaf_diff[which(ldaf_diff$match == F),]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    ldaf1  ldaf2 match\n17    B      A   FALSE\n29    B      A   FALSE\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIndividuals 17 and 29 both differed in species prediction between the model fit and the jackknife posterior probability. Now let's check the accuracy of our model fit:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate error\nldaf_wrong = length(which(ldaf_pred != snake$Species)) # Number of incorrect predictions\nldaf_err = ldaf_wrong/nrow(snake) # Divide by number of individuals for error\n\n# Print error\nldaf_wrong\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\nldaf_err\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1428571\n```\n\n\n:::\n:::\n\n\n\n\n\n\nOur model classified 5 out of 35 (\\~14.3%) of the snakes as the incorrect species, meaning 30/35 were correct (\\~85.7%). Not bad, but can we do better?\n\n## Model Selection\n\nOur previous model used all 6 measurements, but do we really need all of them, or are some of them unhelpful (or even detrimental)? To test this, we can run model selection using the `stepclass()` function:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# stepclass package\nlibrary(klaR)\n\n# Model selection (forward)\nms_f = stepclass(Species ~ M1+M2+M3+M4+M5+M6,data=snake,\n          method=\"lda\", fold=35, direction=\"forward\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n `stepwise classification', using 35-fold cross-validated correctness rate of method lda'.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n35 observations of 6 variables in 2 classes; direction: forward\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nstop criterion: improvement less than 5%.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncorrectness rate: 0.85714;  in: \"M6\";  variables (1): M6 \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.44 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Print model selection result\nms_f\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmethod      : lda \nfinal model : Species ~ M6\n<environment: 0x00000228524a6110>\n\ncorrectness rate = 0.8571 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nAfter model selection, we end up with a model using only M6 to predict species, with a correctness rate of 85.7%. This model has the same correctness as the full model, using only one measurement. In other words, this model is more **efficient** - it gets to the same accuracy using less information.\n\nThis model was generated using forward model selection, meaning the selection process works exclusively by adding variables to the model. We can also do the opposite:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# stepclass package\nlibrary(klaR)\n\n# Model selection (forward)\nms_b = stepclass(Species ~ M1+M2+M3+M4+M5+M6,data=snake,\n          method=\"lda\", fold=35, direction=\"backward\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n `stepwise classification', using 35-fold cross-validated correctness rate of method lda'.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n35 observations of 6 variables in 2 classes; direction: backward\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nstop criterion: improvement less than 5%.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncorrectness rate: 0.8;  starting variables (6): M1, M2, M3, M4, M5, M6 \ncorrectness rate: 0.85714;  out: \"M5\";  variables (5): M1, M2, M3, M4, M6 \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.58 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Print model selection result\nms_b\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmethod      : lda \nfinal model : Species ~ M1 + M2 + M3 + M4 + M6\n<environment: 0x0000022849853cf8>\n\ncorrectness rate = 0.8571 \n```\n\n\n:::\n:::\n\n\n\n\n\n\nBackwards model selection works by removing variables from the full model. This means backwards selection usually returns a model with equal or more variables than forwards selection.\n\nLastly, we can run both:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# stepclass package\nlibrary(klaR)\n\n# Model selection (forward)\nms_d = stepclass(Species ~ M1+M2+M3+M4+M5+M6,data=snake,\n          method=\"lda\", fold=35, direction=\"both\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n `stepwise classification', using 35-fold cross-validated correctness rate of method lda'.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n35 observations of 6 variables in 2 classes; direction: both\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nstop criterion: improvement less than 5%.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncorrectness rate: 0.85714;  in: \"M6\";  variables (1): M6 \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.42 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Print model selection result\nms_d\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmethod      : lda \nfinal model : Species ~ M6\n<environment: 0x00000228478047a0>\n\ncorrectness rate = 0.8571 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Plotting Probabilities\n\nLets finish off by making some plots to visualize our LDA model results.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pick a model to plot\nldaf3 = lda(Species ~ M6, data = snake)\n\n# Plot density curve\nplot(ldaf3, dimen = 1, type = 'dens')\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nThis plots the posterior probabilites of an individual belonging to either species given its LD1 value. Remember from earlier that species A is associated with lower LD1 values.\n\nWe can also make this plot as a histogram:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot density curve\npar(mar = c (4,4,4,4))\nplot(ldaf3, dimen = 1, type = 'hist')\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nOr combine both plots:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot density curve\npar(mar = c (4,4,4,4))\nplot(ldaf3, dimen = 1, type = 'both')\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nAs always, we can also do this with ggplot too:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict species\nldaf3_pred = predict(ldaf3)\n\n# Plot\npred_species = as.data.frame(ldaf3_pred$x) # Gather LD1 values\npred_species$Species = snake$Species # Gather true species from data\n\n# Plot\nggplot(pred_species, aes(x = LD1, fill = Species))+\n  geom_density(alpha = 0.4)# alpha tells you how transparent the plots will be\n```\n\n::: {.cell-output-display}\n![](a1b_files/figure-pdf/unnamed-chunk-17-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n## Tips for your assignment\n\nSome things you may want to think about for your assignment:\n\n1\\. How would you pick which model you think is best? What factors would you consider? Are there any factors you would consider other than those discussed in this tutorial?\n\n2\\. How would you interpret your statistical results biologically (can be in terms of the snakes, how you would study them, or both)? You don't have to be right, but don't be vague, and don't contradict your results.\n",
    "supporting": [
      "a1b_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}