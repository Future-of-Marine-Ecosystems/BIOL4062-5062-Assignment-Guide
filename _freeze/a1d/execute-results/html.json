{
  "hash": "294522983504663cb15f8df59a427265",
  "result": {
    "engine": "knitr",
    "markdown": "# **Assignment 1d:** Multiple Linear Regression\n\nThis assignment is all about multiple linear regression. Linear regression is used to model relationships between a dependent (response) variable and one or more independent (predictor) variables. Multiple linear regression involves multiple predictor variables.\n\nFor this tutorial we're going to use `Schoenemann.csv`, derived from the data in [this](https://doi.org/10.1159/000073759) paper.\n\n## Looking at the data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in data\ndata = read.csv('Schoenemann.csv')\n\n# View data structure\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Order     Family   Genus    Species Location   Mass    Fat   FFWT    CNS\n1 Carnivora    Felidae   Felis canadensis   Alaska 7688.0 1120.0 6568.0 105.09\n2 Carnivora    Felidae   Felis      rufus Virginia 6152.0  738.0 5414.0  81.75\n3 Carnivora Mustelidae    Gulo     luscus   Alaska 9362.0  562.0 8800.0  85.36\n4 Carnivora Mustelidae Mustela    erminea   Alaska  183.3    3.1  180.2   6.69\n5 Carnivora Mustelidae Mustela      vison Virginia 1032.0   66.0  966.0  18.06\n6 Carnivora Proyonidae Procyon      lotor Virginia 6040.0 1013.0 5027.0  58.31\n  HEART  MUSCLE   BONE\n1 27.59 4341.45 631.18\n2 25.45 3600.31 552.23\n3 80.96 5271.20 879.12\n4  1.87  104.70  21.98\n5  7.63  581.53  80.27\n6 36.19 2920.69 517.78\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 39 12\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe Schoenemann dataset contains 39 observations of 12 variables, describing to the morphometry of different species of mammals, along with taxonomic and location information. Let's start by getting rid of the non-morphological data. We won't need it for this assignment.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove metadata\ndata = data[,which(colnames(data) == 'Mass'):ncol(data)] # I do it this way to avoid hard coding a number which may change\n\n# check if it worked\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Mass    Fat   FFWT    CNS HEART  MUSCLE   BONE\n1 7688.0 1120.0 6568.0 105.09 27.59 4341.45 631.18\n2 6152.0  738.0 5414.0  81.75 25.45 3600.31 552.23\n3 9362.0  562.0 8800.0  85.36 80.96 5271.20 879.12\n4  183.3    3.1  180.2   6.69  1.87  104.70  21.98\n5 1032.0   66.0  966.0  18.06  7.63  581.53  80.27\n6 6040.0 1013.0 5027.0  58.31 36.19 2920.69 517.78\n```\n\n\n:::\n:::\n\n\n\n\n\n\nNow we only have the numeric morphologicall data left. Let's take a look at the data graphically.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the data\nboxplot(data)\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Loop through columns to create histograms\nfor(i in 1:ncol(data)){hist(data[,i], main = colnames(data)[i], xlab = \"\")} # Name histogram according to column name\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-3-8.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Considering Transformations\n\nWe can see that these data have many more small values, and the data at higher values has higher variance \\[try e.g. plot(data\\$Mass, data\\$Fat) to see this\\]. If we run our regressions on these data, the assumption of heteroscedasticity is going to be violated:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a test model and check assumptions\ntest = lm(CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, data = data)\n\n# Check for normality as an example\n\n# Residual histogram\nhist(residuals(test), 20)\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# QQplot\nqqnorm(residuals(test))\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Statistical test for normality\nshapiro.test(residuals(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(test)\nW = 0.87473, p-value = 0.0004494\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThese diagnostics look... less than ideal.\n\nThis is a textbook case of when to apply a log transformation to standardize variance - remember logging is the opposite of exponentiating:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply log transformation\ndata_l = log(data)\n\n# Check out the new data\nhead(data_l)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Mass      Fat     FFWT      CNS     HEART   MUSCLE     BONE\n1 8.947416 7.021084 8.789965 4.654817 3.3174534 8.375964 6.447591\n2 8.724533 6.603944 8.596743 4.403666 3.2367157 8.188775 6.313965\n3 9.144414 6.331502 9.082507 4.446878 4.3939552 8.570013 6.778921\n4 5.211124 1.131402 5.194067 1.900614 0.6259384 4.651099 3.090133\n5 6.939254 4.189655 6.873164 2.893700 2.0320878 6.365663 4.385396\n6 8.706159 6.920672 8.522579 4.065774 3.5887828 7.979575 6.249550\n```\n\n\n:::\n\n```{.r .cell-code}\n# Looking at the data\nboxplot(data_l)\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Loop through columns to create histograms\nfor(i in 1:ncol(data_l)){hist(data_l[,i], main = colnames(data_l)[i], xlab = \"\")} # Name histogram according to column name\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-5-8.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow our data looks much more uniform.\n\n**Always remember that transforming your data incorrectly or unnecessarily can do more harm than good.** How do you decide if it is helpful to transform your data? What is the purpose of transforming your data? Think carefully about these questions for your assignment when you're deciding whether to transform any data.\n\n## Simple Linear Regression\n\nNow that our data is good to go, we're going to run some simple linear regressions on the logged data to predict central nervous system mass (CNS). Simple linear regressions only have one predictor variable, so we will run separate models for each predictor. Linear regression is run using the `lm()` command:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run simple linear regressions - Mass\nm1 = lm(CNS ~ Mass, data = data_l) # run model\nsummary(m1) # model summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ Mass, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77785 -0.20227 -0.05439  0.19607  0.78453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.79097    0.14844  -18.80   <2e-16 ***\nMass         0.77105    0.02556   30.16   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3657 on 37 degrees of freedom\nMultiple R-squared:  0.9609,\tAdjusted R-squared:  0.9599 \nF-statistic: 909.7 on 1 and 37 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run simple linear regressions - Fat\nm2 = lm(CNS ~ Fat, data = data_l) # run model\nsummary(m2) # model summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ Fat, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22918 -0.41510  0.01431  0.36008  1.38000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.15712    0.13223  -1.188    0.242    \nFat          0.59903    0.03518  17.028   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6223 on 37 degrees of freedom\nMultiple R-squared:  0.8868,\tAdjusted R-squared:  0.8838 \nF-statistic: 289.9 on 1 and 37 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run simple linear regressions - FFWT\nm3 = lm(CNS ~ FFWT, data = data_l) # run model\nsummary(m3) # model summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ FFWT, data = data_l)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8057 -0.2112 -0.0535  0.1907  0.7654 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.80193    0.14382  -19.48   <2e-16 ***\nFFWT         0.78494    0.02515   31.20   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3539 on 37 degrees of freedom\nMultiple R-squared:  0.9634,\tAdjusted R-squared:  0.9624 \nF-statistic: 973.7 on 1 and 37 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run simple linear regressions - HEART\nm4 = lm(CNS ~ HEART, data = data_l) # run model\nsummary(m4) # model summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ HEART, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75646 -0.16000 -0.03248  0.15018  0.85234 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.99514    0.05853   17.00   <2e-16 ***\nHEART        0.88201    0.02872   30.71   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3594 on 37 degrees of freedom\nMultiple R-squared:  0.9622,\tAdjusted R-squared:  0.9612 \nF-statistic:   943 on 1 and 37 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run simple linear regressions - MUSCLE\nm5 = lm(CNS ~ MUSCLE, data = data_l) # run model\nsummary(m5) # model summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ MUSCLE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82059 -0.15588 -0.00489  0.17331  0.80475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.20579    0.11856  -18.61   <2e-16 ***\nMUSCLE       0.76488    0.02296   33.31   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3323 on 37 degrees of freedom\nMultiple R-squared:  0.9677,\tAdjusted R-squared:  0.9669 \nF-statistic:  1109 on 1 and 37 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run simple linear regressions - BONE\nm6 = lm(CNS ~ BONE, data = data_l) # run model\nsummary(m6) # model summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ BONE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.10309 -0.24611  0.01155  0.25195  0.63931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.0497     0.1042  -10.07 3.75e-12 ***\nBONE          0.7856     0.0277   28.36  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3879 on 37 degrees of freedom\nMultiple R-squared:  0.956,\tAdjusted R-squared:  0.9548 \nF-statistic: 804.4 on 1 and 37 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn this case, it looks like all of our variables are strong, significant predictors with high R^2^ values.\n\nLet's plot all of these regressions:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot regressions\n\n# Plot simple linear regressions - Mass\nplot(CNS ~ Mass, data = data_l, pch = 16) # plot points\nabline(m1, lwd = 2, col = 'red') # Plot model\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot simple linear regressions - Fat\nplot(CNS ~ Fat, data = data_l, pch = 16) # plot points\nabline(m2, lwd = 2, col = 'red') # Plot model\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot simple linear regressions - FFWT\nplot(CNS ~ FFWT, data = data_l, pch = 16) # plot points\nabline(m3, lwd = 2, col = 'red') # Plot model\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot simple linear regressions - HEART\nplot(CNS ~ HEART, data = data_l, pch = 16) # plot points\nabline(m4, lwd = 2, col = 'red') # Plot model\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-7-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot simple linear regressions - MUSCLE\nplot(CNS ~ MUSCLE, data = data_l, pch = 16) # plot points\nabline(m5, lwd = 2, col = 'red') # Plot model\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-7-5.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot simple linear regressions - BONE\nplot(CNS ~ BONE, data = data_l, pch = 16) # plot points\nabline(m6, lwd = 2, col = 'red') # Plot model\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-7-6.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAll of the regression slopes are positive. This makes sense - larger animals tend to have larger brains. Remember to always think about whether your results make biological sense.\n\n## Multiple Linear Regression\n\nWe've made 6 models using 1 variable. Now, let's try making 1 model with 6 variables:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run full model\nm7 = lm(CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, data = data_l)\nsummary(m7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, \n    data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.72690 -0.12073  0.00376  0.08672  0.85638 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.61138    1.18496  -0.516   0.6094  \nMass        -0.46867    2.14250  -0.219   0.8282  \nFat         -0.06818    0.15489  -0.440   0.6628  \nFFWT        -0.02606    2.30347  -0.011   0.9910  \nHEART        0.41894    0.21913   1.912   0.0649 .\nMUSCLE       1.03524    0.61123   1.694   0.1000  \nBONE        -0.06339    0.32054  -0.198   0.8445  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3286 on 32 degrees of freedom\nMultiple R-squared:  0.9727,\tAdjusted R-squared:  0.9676 \nF-statistic: 190.1 on 6 and 32 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\nOur full model has a very(!) high R^2^ value, and, in contrast to the simple linear regressions where every predictor was significant, none of our predictors are considered significant in the final model at $\\alpha$ = 0.05. Why do you think that is?\n\n## Checking Assumptions\n\nNow that we've run our full model, it's time to check its assumptions. Those assumptions are **Independence, Linearity, Homoscedasticity, and Normality.** By now, you should be familiar with what these all mean, but let's run through them anyways:\n\n### Independence\n\nThe assumption of independence states that the value of each data point ('datum', if you will) is independent of all other data points. Some of the ways in which it could be violated may not be testable (e.g. if they have to do with how the data was collected), but what we *can* test for is **autocorrelation**. Autocorrelation translates to self correlation (auto = self). We can test for autocorrelation statistically using a Durbin-Watson test, and visually using an autocorrelation function on the residuals:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: zoo\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'zoo'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n\n\n:::\n\n```{.r .cell-code}\n# Durbin-watson test\ndwtest(m7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tDurbin-Watson test\n\ndata:  m7\nDW = 2.4315, p-value = 0.8545\nalternative hypothesis: true autocorrelation is greater than 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Autocorrelation function\nacf(residuals(m7))\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThe Durbin-Watson test returns an insignificant p-value, indicating no autocorrelation structure is present. The ACF plots the correlation coefficient of the data against itself using lags. Lag 0 correlates the data against itself, which is always 1. Lag 1 correlates each data point against the point after it, and so on. All of the correlation coefficients are between the blue lines, so again, we have no autocorrelation structure, and we can say independence is respected.\n\n### Linearity\n\nThe assumption of linearity states that the response variable consistently scales linearly with its predictors. We can test for linearity statistically using Ramsey's RESET test on our model:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run RESET test\nresettest(m7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tRESET test\n\ndata:  m7\nRESET = 0.12784, df1 = 2, df2 = 30, p-value = 0.8805\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn this case, the p-value is not significant, meaning the assumption of linearity is respected.\n\n### Homoscedasticity\n\nThe assumption of homoscedasticity is that the variance in the data is independent of the value of the data - i.e. the variance in the data is consistent. We can test this statistically using the Breusch-Pagan test, and visually by plotting the model residuals against the fitted values.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run Breusch-Pagan test\nbptest(m7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  m7\nBP = 15.974, df = 6, p-value = 0.01389\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot residuals vs fitted\nplot(m7$residuals ~ m7$fitted.values, pch = 16); abline(h = 0)\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Can also be done using plot.lm, ?plot.lm for details\nplot(m7, 1)\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n\n\n\n\nThe Breusch-Pagan returns a significant p-value, indicating the assumption of homoscedasticity is violated. We can see in the residuals versus fitted plot that the variance in the data is smaller at low values than it is at higher values (the points on the left of the plot are clustered more closely than they are on the right). Let's come back to this later.\n\n### Normality\n\nThe assumption of normality states that the residuals of our model should be normally distributed. If they aren't, that would indicate that our model is biased towards overprediction or underprediction in some way. As we did earlier in the transformation section, we can check for normality visually by looking at histograms and QQ plots of our residuals, and statistically by running a Shapiro-Wilk test on the residuals.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual histogram\nhist(residuals(m7))\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# QQplot\nqqnorm(residuals(m7))\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Can also use plot.lm for qqplot\nplot(m7, 2)\n```\n\n::: {.cell-output-display}\n![](a1d_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Statistical test for normality\nshapiro.test(residuals(m7))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m7)\nW = 0.95391, p-value = 0.1113\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe Shapiro-Wilk test p-value is not significant (though it comes close), meaning the assumption of normality is respected. The residual histogram largely looks normal, and the QQ plot tails start to pull off the line at high and low values, possibly indicating outliers are causing us some trouble, but not enough to violate the assumption.\n\n### What if my assumptions aren't respected?\n\nThe typical fixes for violated assumptions are data transformations, and the removal of outliers. In our case, we pass all assumptions except for homoscedasticity. We've already transformed our data to meet the assumption of normality, so further transformation is likely off the table, though we could potentially try different transformations. We could also try outlier removal - our model diagnostics using `plot.lm()` identify three outliers - 19, 20, and 28. Feel free to play around with removing outliers if you want, although in general it is good practice to only remove outliers where absolutely necessary, as they may contain important biological or other information.\n\nKeep in mind that data transformations and removing outliers both represent trade-offs. Removing outliers may help meet your model assumptions, but you may also be removing data that reflects reality from your model. In that case, is it really helping you to remove outliers? Similarly, transforming your data may help you meet your assumptions, but in a case like this, transforming our data further or in a different way could end up violating other assumptions. Sometimes the best way to deal with violated assumptions is simply to state that they are violated and think about what that means for the interpretation of your model. Play around with all these different ideas, and come up with what you think is best. At the end of the day, a lot of statistical choices are judgement calls, with no perfect right answer.\n\n## Model Selection\n\nIn assignment 1b, we created 1 model with 6 variables, then tested if we could get a similarly effective mode using fewer variables - i.e. a more **efficient** model. Let's do the same thing here:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stepwise model selection - forward\nm8 = step(lm(CNS ~1, data = data_l),\n          scope=(CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE),\n          direction='forward')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=47.92\nCNS ~ 1\n\n         Df Sum of Sq     RSS     AIC\n+ MUSCLE  1    122.51   4.086 -83.984\n+ FFWT    1    121.96   4.634 -79.074\n+ HEART   1    121.82   4.780 -77.869\n+ Mass    1    121.65   4.948 -76.519\n+ BONE    1    121.03   5.567 -71.921\n+ Fat     1    112.27  14.327 -35.056\n<none>                126.595  47.920\n\nStep:  AIC=-83.98\nCNS ~ MUSCLE\n\n        Df Sum of Sq    RSS     AIC\n+ HEART  1  0.233798 3.8522 -84.282\n+ Mass   1  0.212536 3.8735 -84.067\n<none>               4.0860 -83.984\n+ Fat    1  0.152823 3.9332 -83.470\n+ FFWT   1  0.139313 3.9467 -83.337\n+ BONE   1  0.021542 4.0645 -82.190\n\nStep:  AIC=-84.28\nCNS ~ MUSCLE + HEART\n\n       Df Sum of Sq    RSS     AIC\n+ Mass  1   0.35155 3.5007 -86.014\n+ Fat   1   0.29737 3.5549 -85.415\n+ FFWT  1   0.23682 3.6154 -84.756\n<none>              3.8522 -84.282\n+ BONE  1   0.10287 3.7494 -83.337\n\nStep:  AIC=-86.01\nCNS ~ MUSCLE + HEART + Mass\n\n       Df Sum of Sq    RSS     AIC\n<none>              3.5007 -86.014\n+ Fat   1  0.040574 3.4601 -84.468\n+ FFWT  1  0.018388 3.4823 -84.219\n+ BONE  1  0.000458 3.5002 -84.019\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ MUSCLE + HEART + Mass, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74267 -0.11882 -0.00818  0.10790  0.84702 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.1840     0.8704  -0.211  0.83383   \nMUSCLE        1.2436     0.4254   2.923  0.00603 **\nHEART         0.3855     0.1997   1.931  0.06166 . \nMass         -0.8197     0.4372  -1.875  0.06919 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3163 on 35 degrees of freedom\nMultiple R-squared:  0.9723,\tAdjusted R-squared:   0.97 \nF-statistic: 410.2 on 3 and 35 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\nRunning forward model selection cuts the model down to 3 variables. As with 1b, we can also do backward:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stepwise model selection - backwards\nm9 = step(m7, direction = 'backward')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=-80.52\nCNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- FFWT    1   0.00001 3.4552 -82.523\n- BONE    1   0.00422 3.4594 -82.476\n- Mass    1   0.00517 3.4604 -82.465\n- Fat     1   0.02092 3.4761 -82.288\n<none>                3.4552 -80.523\n- MUSCLE  1   0.30975 3.7650 -79.175\n- HEART   1   0.39468 3.8499 -78.305\n\nStep:  AIC=-82.52\nCNS ~ Mass + Fat + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- BONE    1   0.00487 3.4601 -84.468\n- Fat     1   0.04499 3.5002 -84.019\n- Mass    1   0.05110 3.5063 -83.951\n<none>                3.4552 -82.523\n- MUSCLE  1   0.38148 3.8367 -80.439\n- HEART   1   0.39621 3.8514 -80.289\n\nStep:  AIC=-84.47\nCNS ~ Mass + Fat + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n- Fat     1   0.04057 3.5007 -86.014\n- Mass    1   0.09476 3.5549 -85.415\n<none>                3.4601 -84.468\n- HEART   1   0.40303 3.8631 -82.171\n- MUSCLE  1   0.41063 3.8707 -82.095\n\nStep:  AIC=-86.01\nCNS ~ Mass + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n<none>                3.5007 -86.014\n- Mass    1   0.35155 3.8522 -84.282\n- HEART   1   0.37281 3.8735 -84.067\n- MUSCLE  1   0.85479 4.3555 -79.493\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ Mass + HEART + MUSCLE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74267 -0.11882 -0.00818  0.10790  0.84702 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.1840     0.8704  -0.211  0.83383   \nMass         -0.8197     0.4372  -1.875  0.06919 . \nHEART         0.3855     0.1997   1.931  0.06166 . \nMUSCLE        1.2436     0.4254   2.923  0.00603 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3163 on 35 degrees of freedom\nMultiple R-squared:  0.9723,\tAdjusted R-squared:   0.97 \nF-statistic: 410.2 on 3 and 35 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAnd both:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stepwise model selection - both\nm10 = step(m7, direction = 'both')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=-80.52\nCNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- FFWT    1   0.00001 3.4552 -82.523\n- BONE    1   0.00422 3.4594 -82.476\n- Mass    1   0.00517 3.4604 -82.465\n- Fat     1   0.02092 3.4761 -82.288\n<none>                3.4552 -80.523\n- MUSCLE  1   0.30975 3.7650 -79.175\n- HEART   1   0.39468 3.8499 -78.305\n\nStep:  AIC=-82.52\nCNS ~ Mass + Fat + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- BONE    1   0.00487 3.4601 -84.468\n- Fat     1   0.04499 3.5002 -84.019\n- Mass    1   0.05110 3.5063 -83.951\n<none>                3.4552 -82.523\n+ FFWT    1   0.00001 3.4552 -80.523\n- MUSCLE  1   0.38148 3.8367 -80.439\n- HEART   1   0.39621 3.8514 -80.289\n\nStep:  AIC=-84.47\nCNS ~ Mass + Fat + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n- Fat     1   0.04057 3.5007 -86.014\n- Mass    1   0.09476 3.5549 -85.415\n<none>                3.4601 -84.468\n+ BONE    1   0.00487 3.4552 -82.523\n+ FFWT    1   0.00067 3.4594 -82.476\n- HEART   1   0.40303 3.8631 -82.171\n- MUSCLE  1   0.41063 3.8707 -82.095\n\nStep:  AIC=-86.01\nCNS ~ Mass + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n<none>                3.5007 -86.014\n+ Fat     1   0.04057 3.4601 -84.468\n- Mass    1   0.35155 3.8522 -84.282\n+ FFWT    1   0.01839 3.4823 -84.219\n- HEART   1   0.37281 3.8735 -84.067\n+ BONE    1   0.00046 3.5002 -84.019\n- MUSCLE  1   0.85479 4.3555 -79.493\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = CNS ~ Mass + HEART + MUSCLE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74267 -0.11882 -0.00818  0.10790  0.84702 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.1840     0.8704  -0.211  0.83383   \nMass         -0.8197     0.4372  -1.875  0.06919 . \nHEART         0.3855     0.1997   1.931  0.06166 . \nMUSCLE        1.2436     0.4254   2.923  0.00603 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3163 on 35 degrees of freedom\nMultiple R-squared:  0.9723,\tAdjusted R-squared:   0.97 \nF-statistic: 410.2 on 3 and 35 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIf you want to get fancy, we can even look at every possible model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MuMIn)\n\n# Set global options to avoid error\noptions('na.action' = na.fail)\n\n# Run dredge to get full selection table\ndredge(m7, rank = 'AIC')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nFixed term is \"(Intercept)\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGlobal model call: lm(formula = CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, \n    data = data_l)\n---\nModel selection table \n   (Intrc)     BONE       Fat     FFWT  HEART    Mass  MUSCL df  logLik   AIC\n57 -0.1840                             0.3855 -0.8197 1.2440  5  -8.332  26.7\n43 -1.1710          -0.124700          0.3924         0.5758  5  -8.631  27.3\n45 -0.2475                    -0.85860 0.3612         1.2890  5  -8.961  27.9\n59 -0.4891          -0.061790          0.4060 -0.5708 1.0530  6  -8.104  28.2\n47 -0.5634          -0.096360 -0.57650 0.4140         1.0840  6  -8.124  28.2\n41 -1.1430                             0.2965         0.5107  4 -10.198  28.4\n61 -0.3049                     0.58870 0.3869 -1.2410 1.0870  6  -8.229  28.5\n49 -1.7220                                    -0.6191 1.3760  4 -10.305  28.6\n58 -0.2132 -0.01926                    0.3888 -0.8022 1.2420  6  -8.329  28.7\n33 -2.2060                                            0.7649  3 -11.347  28.7\n44 -1.2470 -0.17370 -0.114900          0.4320         0.6975  6  -8.363  28.7\n35 -2.4620          -0.085950                         0.8664  4 -10.603  29.2\n42 -1.2550 -0.24820                    0.3639         0.6919  5  -9.670  29.3\n37 -1.7080                    -0.64360                1.3900  4 -10.670  29.3\n46 -0.3146 -0.03584           -0.80970 0.3672         1.2710  6  -8.953  29.9\n60 -0.6121 -0.06454 -0.066880          0.4188 -0.4915 1.0320  7  -8.077  30.2\n63 -0.4991          -0.070960 -0.17140 0.4086 -0.4111 1.0710  7  -8.101  30.2\n48 -0.6664 -0.05386 -0.097010 -0.50110 0.4234         1.0550  7  -8.106  30.2\n15 -1.3260          -0.141700  0.53690 0.4770                 5 -10.136  30.3\n62 -0.4490 -0.07620            0.72770 0.4004 -1.2710 1.0430  7  -8.195  30.4\n29 -0.9446                     2.32300 0.4383 -1.8930         5 -10.222  30.4\n53 -1.8390                     0.54460        -1.0080 1.2310  5 -10.226  30.5\n50 -1.4810  0.10920                           -0.7284 1.3790  5 -10.226  30.5\n34 -2.3590 -0.10740                                   0.8684  4 -11.244  30.5\n51 -1.9150          -0.030810                 -0.4897 1.2850  5 -10.253  30.5\n27 -1.3680          -0.174800          0.4968  0.5506         5 -10.291  30.6\n39 -2.0530          -0.062690 -0.43960                1.2660  5 -10.335  30.7\n36 -2.5030 -0.03440 -0.083230                         0.8963  5 -10.593  31.2\n38 -1.4490  0.10160           -0.79230                1.4370  5 -10.612  31.2\n16 -1.6470 -0.20880 -0.139600  0.71490 0.5072                 6  -9.877  31.8\n30 -1.3160 -0.23860            2.54100 0.4741 -1.9060         6  -9.885  31.8\n13 -1.0500                     0.42240 0.4110                 4 -11.885  31.8\n31 -1.1730          -0.087570  1.35400 0.4642 -0.8572         6 -10.045  32.1\n64 -0.6114 -0.06339 -0.068180 -0.02606 0.4189 -0.4687 1.0350  8  -8.077  32.2\n28 -1.6410 -0.16770 -0.182100          0.5254  0.6983         6 -10.119  32.2\n54 -1.6320  0.07991            0.40060        -0.9854 1.2720  6 -10.189  32.4\n52 -1.6650  0.09609 -0.024700                 -0.6115 1.3050  6 -10.194  32.4\n55 -1.8600          -0.005303  0.48770        -0.9454 1.2310  6 -10.225  32.5\n40 -1.7890  0.10350 -0.062910 -0.59040                1.3130  6 -10.273  32.5\n21 -2.8120                     2.53800        -1.7250         4 -12.557  33.1\n25 -0.7492                             0.4796  0.3550         4 -12.592  33.2\n14 -1.4090 -0.23040            0.62070 0.4454                 5 -11.597  33.2\n32 -1.4890 -0.22300 -0.075840  1.68700 0.4941 -1.0080         7  -9.751  33.5\n5  -2.8020                     0.78490                        3 -13.802  33.6\n7  -3.2380          -0.110900  0.92010                        4 -12.820  33.6\n56 -1.6680  0.08319 -0.011140  0.27490        -0.8521 1.2720  7 -10.186  34.4\n19 -3.4510          -0.166800                  0.9721         4 -13.253  34.5\n9   0.9951                             0.8820                 3 -14.404  34.8\n12  0.3485  0.33210 -0.111800          0.6667                 5 -12.415  34.8\n10  0.3284  0.25560                    0.5985                 4 -13.429  34.9\n22 -3.0100 -0.08987            2.62700        -1.7240         5 -12.513  35.0\n23 -2.8680          -0.014310  2.38200        -1.5540         5 -12.553  35.1\n26 -0.8465 -0.06583                    0.4905  0.4097         5 -12.568  35.1\n6  -3.0010 -0.09063            0.87490                        4 -13.760  35.5\n8  -3.3620 -0.05836 -0.109700  0.97660                        5 -12.802  35.6\n11  1.1350          -0.071490          0.9798                 4 -13.981  36.0\n17 -2.7910                                     0.7710         3 -15.079  36.2\n20 -3.4240  0.01165 -0.166400                  0.9601         5 -13.252  36.5\n24 -3.0350 -0.08761 -0.007846  2.53900        -1.6310         6 -12.511  37.0\n18 -2.5850  0.09433                            0.6790         4 -15.033  38.1\n2  -1.0500  0.78560                                           3 -17.378  40.8\n4  -1.1140  0.84970 -0.052420                                 4 -17.190  42.4\n3  -0.1571           0.599000                                 3 -35.811  77.6\n1   1.3230                                                    2 -78.299 160.6\n    delta weight\n57   0.00  0.104\n43   0.60  0.077\n45   1.26  0.056\n59   1.55  0.048\n47   1.58  0.047\n41   1.73  0.044\n61   1.79  0.042\n49   1.95  0.039\n58   1.99  0.038\n33   2.03  0.038\n44   2.06  0.037\n35   2.54  0.029\n42   2.68  0.027\n37   2.68  0.027\n46   3.24  0.021\n60   3.49  0.018\n63   3.54  0.018\n48   3.55  0.018\n15   3.61  0.017\n62   3.73  0.016\n29   3.78  0.016\n53   3.79  0.016\n50   3.79  0.016\n34   3.82  0.015\n51   3.84  0.015\n27   3.92  0.015\n39   4.01  0.014\n36   4.52  0.011\n38   4.56  0.011\n16   5.09  0.008\n30   5.11  0.008\n13   5.11  0.008\n31   5.43  0.007\n64   5.49  0.007\n28   5.57  0.006\n54   5.71  0.006\n52   5.72  0.006\n55   5.79  0.006\n40   5.88  0.005\n21   6.45  0.004\n25   6.52  0.004\n14   6.53  0.004\n32   6.84  0.003\n5    6.94  0.003\n7    6.98  0.003\n56   7.71  0.002\n19   7.84  0.002\n9    8.14  0.002\n12   8.17  0.002\n10   8.19  0.002\n22   8.36  0.002\n23   8.44  0.002\n26   8.47  0.002\n6    8.86  0.001\n8    8.94  0.001\n11   9.30  0.001\n17   9.49  0.001\n20   9.84  0.001\n24  10.36  0.001\n18  11.40  0.000\n2   14.09  0.000\n4   15.72  0.000\n3   50.96  0.000\n1  133.93  0.000\nModels ranked by AIC(x) \n```\n\n\n:::\n:::\n\n\n\n\n\n\nHere, we're ranking models by AIC. AIC balances fit with model complexity. Lower values of AIC are considered better. Generally, 2 is used as a rule of thumb for delta AIC: if delta AIC is \\>2, the model with the higher AIC value has little support. If delta AIC is \\<2, there is at least some support for the model with the higher AIC.\n\n## Tips for your Assignment:\n\nSome things you may want to think about for your assignment:\n\n1.  What role is collinearity playing in your assignment? Is it something you should be concerned about? Why or why not?\n\n2.  What does it mean if your assumptions are violated? How would you fix it? Is it worth fixing it? Why or why not?\n\n3.  How would you interpret your statistical results biologically? You don't have to be right, but don't be vague, and don't contradict your results.\n",
    "supporting": [
      "a1d_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}