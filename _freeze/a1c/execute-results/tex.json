{
  "hash": "5394ab4078095c89d22033c368eb5771",
  "result": {
    "markdown": "# **Assignment 1c:** Cluster Analysis and Multidimensional Scaling\n\nThis assignment is centered on cluster analysis and multidimensional scaling (MDS), which are both methods of measuring associations within a group (e.g. associations between individuals within a population).\n\nFor this tutorial, we'll be using `monkey.csv`.\n\n## Looking at the data\n\nYou know the drill by now:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load in data\ndata = read.csv('monkey.csv', row.names = 1) # First column is row names\ndata # Print data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      ind1 ind2 ind3 ind4 ind5 ind6 ind7 ind8 ind9 ind10 ind11 ind12 ind13\nind1    21    2    2   10    2    2    8    0    0     8    14    12     4\nind2     2   21   16    2   16    8    2    2    4     4     4     0     2\nind3     2   16   21    0   10   16    2    0    2     4     4     0     2\nind4    10    2    0   21    2    2   16    2    2     8    12     8     4\nind5     2   16   10    2   21   10    2    4    0     2     4     4     2\nind6     2    8   16    2   10   21    4    2    0     0     0     4     4\nind7     8    2    2   16    2    4   21    4    2    16     8     8     4\nind8     0    2    0    2    4    2    4   21    0     2     0     0     0\nind9     0    4    2    2    0    0    2    0   21     0     4     0     0\nind10    8    4    4    8    2    0   16    2    0    21    14    14     2\nind11   14    4    4   12    4    0    8    0    4    14    21    12     4\nind12   12    0    0    8    4    4    8    0    0    14    12    21     2\nind13    4    2    2    4    2    4    4    0    0     2     4     2    21\n```\n:::\n:::\n\n\n\nOur data is a matrix containing the number of social interactions observed between individuals in a group of monkeys at the zoo. The matrix is symmetrical - the top/right half is identical to the bottom/left half.\n\n## Calculating Dissimilarity\n\nFor this assignment we'll be using 3 R functions: `hclust`, `metaMDS` (from the `vegan` package), `isoMDS` (from the `MASS`package), and `cmdscale()`. Let's see what type of input data those functions need:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check help functions\nlibrary(vegan)\nlibrary(MASS)\n?hclust()\n?metaMDS()\n?isoMDS()\n?cmdscale()\n```\n:::\n\n\n\nYou'll notice all of these functions require a **dissimilarity matrix** produced by `dist`. Let's start by running `dist()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert data to a dist object\ndist = as.dist(data)\ndist # Print dist\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      ind1 ind2 ind3 ind4 ind5 ind6 ind7 ind8 ind9 ind10 ind11 ind12\nind2     2                                                          \nind3     2   16                                                     \nind4    10    2    0                                                \nind5     2   16   10    2                                           \nind6     2    8   16    2   10                                      \nind7     8    2    2   16    2    4                                 \nind8     0    2    0    2    4    2    4                            \nind9     0    4    2    2    0    0    2    0                       \nind10    8    4    4    8    2    0   16    2    0                  \nind11   14    4    4   12    4    0    8    0    4    14            \nind12   12    0    0    8    4    4    8    0    0    14    12      \nind13    4    2    2    4    2    4    4    0    0     2     4     2\n```\n:::\n:::\n\n\n\nNow our data is in a `dist` object. All of the redundant entries in the data have been removed.\n\nRight now, our data reflects **similarity (i.e. high numbers reflect greater association between individuals)**. We need to convert it to **dissimilarity**. Dissimilarity is simply the opposite of similarity. We can convert similarity to dissimilarity by subtracting each data value from the maximum of the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to dissimilarity\ndist = max(dist) - dist\ndist # Print dist\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      ind1 ind2 ind3 ind4 ind5 ind6 ind7 ind8 ind9 ind10 ind11 ind12\nind2    14                                                          \nind3    14    0                                                     \nind4     6   14   16                                                \nind5    14    0    6   14                                           \nind6    14    8    0   14    6                                      \nind7     8   14   14    0   14   12                                 \nind8    16   14   16   14   12   14   12                            \nind9    16   12   14   14   16   16   14   16                       \nind10    8   12   12    8   14   16    0   14   16                  \nind11    2   12   12    4   12   16    8   16   12     2            \nind12    4   16   16    8   12   12    8   16   16     2     4      \nind13   12   14   14   12   14   12   12   16   16    14    12    14\n```\n:::\n:::\n\n\n\nNow we're ready to run our analyses!\n\n## Hierarchical Cluster Analysis\n\nRemember from lecture there are 4 types of hierarchical cluster analysis:\n\n1.  Single linkage\n2.  Average linkage\n3.  Complete linkage\n4.  Ward linkage\n\nLet's run through them one by one:\n\n### Single linkage\n\nWe can run all 4 types of cluster analysis using the `hclust()` R function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run single linkage cluster analysis\nclust_1 = hclust(dist, method = 'single')\nclust_1 # print object\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nhclust(d = dist, method = \"single\")\n\nCluster method   : single \nNumber of objects: 13 \n```\n:::\n:::\n\n\n\nPrinting the `hclust` object doesn't really tell us much. For more detail, we're going to have to plot it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot single linkage tree\nplot(clust_1, hang = -1, main = 'Single linkage', \n     ylab = 'Dissimilarity', # Label y axis\n     xlab = '', sub = '') # Remove x-axis label\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis outputs a tree showing the associations between our individual monkeys. dissimilarity is on the y-axis. The greater the distance between individuals on the y-axis, the greater their dissimilarity. Our tree has grouped the monkeys according to how frequently they interact with each other. For example. individuals 2, 3, 5, and 6 interact often, as evidenced by their low dissimilarity.\n\nBut how well does this tree fit the data? To answer that question, we need to calculate the cophenetic correlation coefficient (CCC):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate CCC\ncoph_1 = cophenetic(clust_1) # Get cophenetic\nccc_1 = cor(coph_1, dist) # Calculate correlation of the cophenetic with the data\nccc_1 # Print CCC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9036043\n```\n:::\n:::\n\n\n\nThat's a pretty high correlation coefficient, indicating our dendrogram represented the structure in the original data very well. Let's try some other methods:\n\n### Average Linkage\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run cluster analysis\nclust_2 = hclust(dist, method = 'average')\n\n# Plot\nplot(clust_2, hang = -1, main = 'Average linkage', ylab = 'Dissimilarity', xlab = '', sub = '')\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Calculate CCC\ncoph_2 = cophenetic(clust_2)\nccc_2 = cor(coph_2, dist)\nccc_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9288949\n```\n:::\n:::\n\n\n\n### Complete Linkage\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run cluster analysis\nclust_3 = hclust(dist, method = 'complete')\n\n# Plot\nplot(clust_3, hang = -1, main = 'Complete linkage', ylab = 'Dissimilarity', xlab = '', sub = '')\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Calculate CCC\ncoph_3 = cophenetic(clust_3)\nccc_3 = cor(coph_3, dist)\nccc_3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9141956\n```\n:::\n:::\n\n\n\n### Ward Linkage\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run cluster analysis\nclust_4 = hclust(dist, method = 'ward.D')\n\n# Plot\nplot(clust_4, hang = -1, main = 'Ward linkage', ylab = 'Dissimilarity', xlab = '', sub = '')\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Calculate CCC\ncoph_4 = cophenetic(clust_4)\nccc_4 = cor(coph_4, dist)\nccc_4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7633159\n```\n:::\n:::\n\n\n\nEach method gives a slightly different tree and CCC value. Where are they similar? Where do they differ? Which one(s) would you trust? Why?\n\n## Multidimensional Scaling\n\nAnother method we can use to test for associations between our monkeys is multidimensional scaling (MDS). There are two types of MDS: non-metric, and metric MDS. Let's start with non-metric MDS.\n\n### Non-Metric MDS\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run non-metric MDS - metaMDS\nmds1 = metaMDS(dist, wascores = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRun 0 stress 0.07592385 \nRun 1 stress 0.1910216 \nRun 2 stress 0.08055013 \nRun 3 stress 0.1193093 \nRun 4 stress 0.07239303 \n... New best solution\n... Procrustes: rmse 0.2256261  max resid 0.6643301 \nRun 5 stress 0.07575137 \nRun 6 stress 0.08055016 \nRun 7 stress 0.08571815 \nRun 8 stress 0.07239301 \n... New best solution\n... Procrustes: rmse 0.0001938111  max resid 0.0004123639 \n... Similar to previous best\nRun 9 stress 0.1172799 \nRun 10 stress 0.07239299 \n... New best solution\n... Procrustes: rmse 6.641911e-05  max resid 0.0001471194 \n... Similar to previous best\nRun 11 stress 0.1396354 \nRun 12 stress 0.07239302 \n... Procrustes: rmse 5.795996e-05  max resid 0.000125756 \n... Similar to previous best\nRun 13 stress 0.07239304 \n... Procrustes: rmse 0.0001081139  max resid 0.0002395363 \n... Similar to previous best\nRun 14 stress 0.07239302 \n... Procrustes: rmse 8.57314e-05  max resid 0.0001896311 \n... Similar to previous best\nRun 15 stress 0.0757299 \nRun 16 stress 0.08055013 \nRun 17 stress 0.07366297 \nRun 18 stress 0.2188633 \nRun 19 stress 0.072393 \n... Procrustes: rmse 9.964643e-06  max resid 1.61464e-05 \n... Similar to previous best\nRun 20 stress 0.179657 \n*** Best solution repeated 5 times\n```\n:::\n\n```{.r .cell-code}\n# Print mds results\nmds1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nmetaMDS(comm = dist, wascores = F) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     dist \nDistance: user supplied \n\nDimensions: 2 \nStress:     0.07239299 \nStress type 1, weak ties\nBest solution was repeated 5 times in 20 tries\nThe best solution was from try 10 (random start)\nScaling: centring, PC rotation \nSpecies: scores missing\n```\n:::\n:::\n\n\n\nBy default, metaMDS has two dimensions. This MDS has a stress value of 0.072. Remember from lecture that stress \\< 0.10 is a \"good representation\", so this MDS result is pretty good. If we want, we can test different numbers of dimensions (k) and create a scree plot to find the best one:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a container object\nscree = data.frame(k = 1:5, stress = NA)\n\n# Loop through k 1 to 5\nfor(k in 1:5){\n  \n  # Run MDS\n  mds = metaMDS(dist, wascores = F, k = k) # Set k to our loop index\n  \n  # Pull out stress\n  scree[k,'stress'] = mds$stress # Fill kth row of the column 'stress' in scree\n  \n} # End loop\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print results\nscree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  k       stress\n1 1 2.360764e-01\n2 2 7.239300e-02\n3 3 9.083008e-05\n4 4 3.593622e-04\n5 5 9.015044e-05\n```\n:::\n\n```{.r .cell-code}\n# Make scree plot\nplot(stress ~ k, data = scree, # Plot stress against k\n     type = 'b', # Lines and points\n     pch = 16) # Point 16 (filled circle)\nabline(h = 0.1, lty = 'dashed') # Plot a dashed line at 0.1\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-13-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWe have an elbow at k=3, but we also get warnings that our dataset may be too small using k=3. The stress at k=2 is low enough that we can stick to using that.\n\nLet's plot our results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot result\nplot(mds1, type = 't')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nspecies scores not available\n```\n:::\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nHere we've plotted the values of our two MDS dimensions against each other for each individual. Similar to the cluster analysis, we see certain individuals are grouped together. Is it the same groups of individuals? What does that tell you about your results?\n\nLet's try a different non-metric MDS function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run non-metric MDS - isoMDS\nmds2 = isoMDS(dist)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in isoMDS(dist): zero or negative distance between objects 2 and 3\n```\n:::\n:::\n\n\n\nUh oh. This function doesn't like zeroes in the data. Let's fix that by translating our data to proportions, and adding a small increment.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Translate to proportions\ndist2 = dist/max(dist)\n\n# Add an increment\ndist2 = dist2 + 0.0001\n\n# Print new dist\ndist2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        ind1   ind2   ind3   ind4   ind5   ind6   ind7   ind8   ind9  ind10\nind2  0.8751                                                               \nind3  0.8751 0.0001                                                        \nind4  0.3751 0.8751 1.0001                                                 \nind5  0.8751 0.0001 0.3751 0.8751                                          \nind6  0.8751 0.5001 0.0001 0.8751 0.3751                                   \nind7  0.5001 0.8751 0.8751 0.0001 0.8751 0.7501                            \nind8  1.0001 0.8751 1.0001 0.8751 0.7501 0.8751 0.7501                     \nind9  1.0001 0.7501 0.8751 0.8751 1.0001 1.0001 0.8751 1.0001              \nind10 0.5001 0.7501 0.7501 0.5001 0.8751 1.0001 0.0001 0.8751 1.0001       \nind11 0.1251 0.7501 0.7501 0.2501 0.7501 1.0001 0.5001 1.0001 0.7501 0.1251\nind12 0.2501 1.0001 1.0001 0.5001 0.7501 0.7501 0.5001 1.0001 1.0001 0.1251\nind13 0.7501 0.8751 0.8751 0.7501 0.8751 0.7501 0.7501 1.0001 1.0001 0.8751\n       ind11  ind12\nind2               \nind3               \nind4               \nind5               \nind6               \nind7               \nind8               \nind9               \nind10              \nind11              \nind12 0.2501       \nind13 0.7501 0.8751\n```\n:::\n:::\n\n\n\nLet's make sure this doesn't mess with our results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run non-metric MDS - metaMDS\nmds1 = metaMDS(dist2, wascores = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRun 0 stress 0.07575137 \nRun 1 stress 0.0757299 \n... New best solution\n... Procrustes: rmse 0.007459314  max resid 0.02401492 \nRun 2 stress 0.07366297 \n... New best solution\n... Procrustes: rmse 0.1937481  max resid 0.5655442 \nRun 3 stress 0.1180402 \nRun 4 stress 0.08573308 \nRun 5 stress 0.1180402 \nRun 6 stress 0.0757299 \nRun 7 stress 0.07575137 \nRun 8 stress 0.07575137 \nRun 9 stress 0.1180402 \nRun 10 stress 0.07875788 \nRun 11 stress 0.0757299 \nRun 12 stress 0.07875788 \nRun 13 stress 0.0856906 \nRun 14 stress 0.07575137 \nRun 15 stress 0.1183846 \nRun 16 stress 0.07227846 \n... New best solution\n... Procrustes: rmse 0.1728511  max resid 0.5161582 \nRun 17 stress 0.07358653 \nRun 18 stress 0.07575137 \nRun 19 stress 0.07575137 \nRun 20 stress 0.07366297 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     8: stress ratio > sratmax\n    12: scale factor of the gradient < sfgrmin\n```\n:::\n\n```{.r .cell-code}\n# Print mds results\nmds1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nmetaMDS(comm = dist2, wascores = F) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     dist2 \nDistance: user supplied \n\nDimensions: 2 \nStress:     0.07227846 \nStress type 1, weak ties\nBest solution was not repeated after 20 tries\nThe best solution was from try 16 (random start)\nScaling: centring, PC rotation \nSpecies: scores missing\n```\n:::\n\n```{.r .cell-code}\n# Plot result\nplot(mds1, type = 't')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nspecies scores not available\n```\n:::\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-17-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThe values have shifted around a bit but the structure and interpretation of the plot is the same. Let's continue on:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run non-metric MDS - isoMDS\nmds2 = isoMDS(dist2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninitial  value 24.760322 \niter   5 value 14.153502\niter  10 value 12.254154\niter  15 value 11.639473\niter  20 value 11.360460\nfinal  value 11.341572 \nconverged\n```\n:::\n\n```{.r .cell-code}\n# Print output\nmds2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$points\n            [,1]         [,2]\nind1   0.5060798  0.103253718\nind2  -0.6157097 -0.285522725\nind3  -0.6133549 -0.310223762\nind4   0.5008471  0.144443835\nind5  -0.6264450 -0.279477605\nind6  -0.6228134 -0.325577873\nind7   0.4940123  0.147103149\nind8  -0.6783876  0.680257989\nind9  -0.2575043  1.075686777\nind10  0.5482152  0.033286470\nind11  0.5478840  0.082682890\nind12  0.5895070  0.001005794\nind13  0.2276697 -1.066918657\n\n$stress\n[1] 11.34157\n```\n:::\n:::\n\n\n\nThe modelling algorithms seems to be a little different, and we end up with a different stress result - in this case, one that is above the 10% threshold (note that stress is in % in this function, unlike metaMDS where it is in proportion). Let's try another scree plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a container object\nscree = data.frame(k = 1:5, stress = NA)\n\n# Loop through k 1 to 5\nfor(k in 1:5){\n  \n  # Run MDS\n  mds = isoMDS(dist2, k = k) # Set k to our loop index\n  \n  # Pull out stress\n  scree[k,'stress'] = mds$stress # Fill kth row of the column 'stress' in scree\n  \n} # End loop\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print results\nscree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  k      stress\n1 1 36.54857293\n2 2 11.34157190\n3 3  0.04614441\n4 4  0.12630298\n5 5  0.19439990\n```\n:::\n\n```{.r .cell-code}\n# Make scree plot\nplot(stress ~ k, data = scree, # Plot stress against k\n     type = 'b', # Lines and points\n     pch = 16) # Point 16 (filled circle)\nabline(h = 10, lty = 'dashed') # Plot a dashed line at 0.1\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nIn this case, it seems we're better off using 3 dimensions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run non-metric MDS - isoMDS\nmds2 = isoMDS(dist2, k = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninitial  value 18.960422 \niter   5 value 11.725940\niter  10 value 6.417141\niter  15 value 4.149185\niter  20 value 1.466748\niter  25 value 0.764657\niter  30 value 0.449114\niter  35 value 0.302911\niter  40 value 0.156116\niter  45 value 0.087536\niter  50 value 0.046144\nfinal  value 0.046144 \nstopped after 50 iterations\n```\n:::\n\n```{.r .cell-code}\n# Print output\nmds2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$points\n            [,1]       [,2]       [,3]\nind1   2.0028765  0.4079841 -0.4202376\nind2  -2.0901193 -1.3657278  1.2720375\nind3  -2.0910832 -1.3666943  1.2729356\nind4   2.0066696  0.4042391 -0.4161698\nind5  -2.0896284 -1.3631915  1.2769256\nind6  -2.0921681 -1.3637290  1.2724192\nind7   2.0032691  0.4111298 -0.4185160\nind8  -2.1600174  2.2033842 -1.8938700\nind9  -0.6641520  2.9972846  2.5707229\nind10  2.0009095  0.4046552 -0.4185896\nind11  2.0041166  0.4044773 -0.4197885\nind12  2.0019583  0.4023552 -0.4223419\nind13 -0.8326313 -2.1761669 -3.2555274\n\n$stress\n[1] 0.04614441\n```\n:::\n:::\n\n\n\nLet's plot our results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot isoMDS\nplot(mds2$points[,1], mds2$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS') # Labelling\n\n# Plot individual names\ntext(mds2$points[,1], mds2$points[,2], rownames(data))\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nAll of our grouped individuals are plotted on top of each other. Let's try adding some random jiggle so we can see them\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot isoMDS\nplot(mds2$points[,1], mds2$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS', # Labelling\n     xlim = c(-2.5, 2.5), ylim = c(3, -3)) # Set axis limits\n\n# Set random seed for consistency\nset.seed(1212)\n\n# Plot individual names\ntext(mds2$points[,1] + rnorm(13, 0, 0.2), # Add random values pulled from a \n     mds2$points[,2] + rnorm(13, 0, 0.2), # normal distribution with mean 0, sd 0.2\n     rownames(data)) # Add names\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-23-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThat's a bit better. We can also add some color to this plot if we want - say, individuals 6 to 9 are juveniles:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot isoMDS\nplot(mds2$points[,1], mds2$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS', # Labelling\n     xlim = c(-2.5, 2.5), ylim = c(3, -3)) # Set axis limits\n\n# Set random seed for consistency\nset.seed(1212)\n\n# Juvenile identifier\nad = c(rep(1,5), rep(0,4), rep(1,4)) # ad is 1 for first 5 and last 4\nad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 0 0 0 0 1 1 1 1\n```\n:::\n\n```{.r .cell-code}\n# Plot individual names\ntext(mds2$points[,1] + rnorm(13, 0, 0.2), # Add random values pulled from a \n     mds2$points[,2] + rnorm(13, 0, 0.2), # normal distribution with mean 0, sd 0.2\n     rownames(data), # Add names\n     col = ifelse(ad == 0, 'purple', 'orange')) # color \n# Add a legend\nlegend('topright', legend = c('Juvenile', 'Adult'), fill = c('purple', 'orange'))\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-24-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nDoes this plot match the previous one, and/or the cluster analyses?\n\n### Metric MDS\n\nWe can run metric MDS using the `cmdscale()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run metric MDS\nmds3 = cmdscale(dist, eig = T)\nmds3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$points\n             [,1]       [,2]\nind1   5.84977914  2.7981654\nind2  -7.46899061 -0.4452479\nind3  -7.87360160  2.4742095\nind4   6.04970828 -1.1964346\nind5  -6.77887791  2.8518073\nind6  -7.21161499  3.9150940\nind7   4.79289905 -0.9896933\nind8  -2.46317365 -5.3304368\nind9  -1.86216019 -9.7770302\nind10  5.45394235  1.1317599\nind11  5.27120921 -0.1097836\nind12  6.20052885  3.6063451\nind13  0.04035206  1.0712450\n\n$eig\n [1]  4.150450e+02  1.794715e+02  1.684147e+02  1.309366e+02  6.931537e+01\n [6]  5.811388e+01  3.306204e+01  1.780496e+01 -4.263256e-14 -8.643935e+00\n[11] -2.208342e+01 -4.468321e+01 -7.952271e+01\n\n$x\nNULL\n\n$ac\n[1] 0\n\n$GOF\n[1] 0.4844901 0.5545014\n```\n:::\n:::\n\n\n\nMetric MDS doesn't have stress. Instead, we have to look at goodness of fit (GOF) to assess how well the analysis worked. GOF is similar to an R^2^ value, where numbers closer to 1 indicate a better fit (though be wary of overfitting!). There are two different GOF values for each metric MDS.\n\nAs with the other MDS functions, k defaults to 2. We can make another scree plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a container object\nscree = data.frame(k = 1:5, GOF1 = NA, GOF2 = NA)\n\n# Loop through k 1 to 5\nfor(k in 1:5){\n  \n  # Run MDS\n  mds = cmdscale(dist, eig = T, k = k) # Set k to our loop index\n  \n  # Pull out stress\n  scree[k,c(2,3)] = mds$GOF # Fill kth row of the GOF columns in scree\n  \n} # End loop\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print results\nscree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  k      GOF1      GOF2\n1 1 0.3382331 0.3871096\n2 2 0.4844901 0.5545014\n3 3 0.6217365 0.7115806\n4 4 0.7284408 0.8337043\n5 5 0.7849281 0.8983543\n```\n:::\n\n```{.r .cell-code}\n# Make scree plot\nplot(GOF2 ~ k, data = scree, # Plot stress against k\n     type = 'b', # Lines and points\n     pch = 16, # Point 16 (filled circle)\n     ylab = 'Goodness of Fit', ylim = c(0.3, 1))\npoints(GOF1 ~ k, data = scree, type = 'b', pch = 16, col = 'red') # Add second GOF value\nabline(h = 0.1, lty = 'dashed') # Plot a dashed line at 0.1\nlegend('topleft', pch = 16, legend = c('GOF1', 'GOF2'), col = c('red', 'black')) # Add legend\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nGoodness of fit scales linearly, so what k to use is more of a judgement call.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run metric MDS\nmds3 = cmdscale(dist, k=4, eig = T)\nmds3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$points\n             [,1]       [,2]       [,3]       [,4]\nind1   5.84977914  2.7981654  1.2685787 -0.6738375\nind2  -7.46899061 -0.4452479  2.6496404  2.1091477\nind3  -7.87360160  2.4742095  3.4326421  1.2339175\nind4   6.04970828 -1.1964346 -0.9306802 -1.2319775\nind5  -6.77887791  2.8518073 -1.2465315  2.3989342\nind6  -7.21161499  3.9150940 -1.9699687 -2.4369605\nind7   4.79289905 -0.9896933 -2.7067201 -0.2043847\nind8  -2.46317365 -5.3304368 -9.4034890  2.0126521\nind9  -1.86216019 -9.7770302  5.2905629 -1.1532342\nind10  5.45394235  1.1317599  0.3565324  4.1657272\nind11  5.27120921 -0.1097836  4.1678975  1.4238728\nind12  6.20052885  3.6063451 -0.2995254  1.5298300\nind13  0.04035206  1.0712450 -0.6089393 -9.1736869\n\n$eig\n [1]  4.150450e+02  1.794715e+02  1.684147e+02  1.309366e+02  6.931537e+01\n [6]  5.811388e+01  3.306204e+01  1.780496e+01 -4.263256e-14 -8.643935e+00\n[11] -2.208342e+01 -4.468321e+01 -7.952271e+01\n\n$x\nNULL\n\n$ac\n[1] 0\n\n$GOF\n[1] 0.7284408 0.8337043\n```\n:::\n:::\n\n\n\nLet's plot the first two dimensions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot metric MDS\nplot(mds3$points[,1], mds3$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS') # Labelling\n\n# Plot individual names\ntext(mds3$points[,1], # Add random values pulled from a \n     mds3$points[,2], # normal distribution with mean 0, sd 0.2\n     rownames(data), # Add names\n     col = ifelse(ad == 0, 'purple', 'orange')) # color \n# Add a legend\nlegend('bottomright', legend = c('Juvenile', 'Adult'), fill = c('purple', 'orange'))\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-29-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### 3D Plotting (Optional)\n\nIt may not be necessary, but if your MDS has more than 2 dimensions, you can try plotting it in three dimensions and see if it helps:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plot3D)\n\n# Prepare data to plot\nx = mds3$points[,1]\ny = mds3$points[,2]\nz = mds3$points[,3]\n\n# Create 3D plot\nscatter3D(x,y,z, colvar = NULL, col = 'blue', \n          pch = 16, cex = 0.5, bty = 'g', theta = 5)\n\n# Add text\ntext3D(x, \n       # Add some jiggle to the labels\n       y+rnorm(13, mean = 0, sd = 0.5), z + rnorm(13, mean = 0, sd = 0.5), \n                    labels = names(mds3$points[,1]), add = T, colkey = F, \n                    cex = 0.5, adj = 1, d = 2)\n```\n\n::: {.cell-output-display}\n![](a1c_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Mantel Test (Graduate Students Only)\n\nWe can infer to some extent whether juveniles and adults preferentially associate with each other from our colored MDS plots, but we can also test it statistically using a Mantel test. To run the Mantel test, we need to convert our adult index into a `dist` object:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create dist matrix for adults\nad_dist = dist(ad)\nad_dist\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1 2 3 4 5 6 7 8 9 10 11 12\n2  0                         \n3  0 0                       \n4  0 0 0                     \n5  0 0 0 0                   \n6  1 1 1 1 1                 \n7  1 1 1 1 1 0               \n8  1 1 1 1 1 0 0             \n9  1 1 1 1 1 0 0 0           \n10 0 0 0 0 0 1 1 1 1         \n11 0 0 0 0 0 1 1 1 1  0      \n12 0 0 0 0 0 1 1 1 1  0  0   \n13 0 0 0 0 0 1 1 1 1  0  0  0\n```\n:::\n:::\n\n\n\nNote this is dissimilarity: adult-juvenile pairs are assigned 1, and same-class pairs are assigned 0.\n\nThe Mantel test looks for correlation between this matrix and our original dissociation matrix, and statistically tests if the associations are different from what we would expect due to chance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run mantel test\nlibrary(ade4)\nmantel.rtest(ad_dist, dist, nrepet = 999)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in is.euclid(m1): Zero distance(s)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in is.euclid(m2): Zero distance(s)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMonte-Carlo test\nCall: mantelnoneuclid(m1 = m1, m2 = m2, nrepet = nrepet)\n\nObservation: 0.1686576 \n\nBased on 999 replicates\nSimulated p-value: 0.073 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n 1.369210491 -0.001062026  0.015364686 \n```\n:::\n:::\n\n\n\nIt's very close, but we don't have statistically significant evidence that juveniles and adults associate preferentially with each other in this case.\n\n## Tips for your Assignment:\n\nSome things you may want to think about for your assignment:\n\n1\\. How would you pick which cluster analyses and MDS analyses are best for your data? Are they conceptual, or do they have to do with the results? Do they agree?\n\n2\\. How would you interpret your statistical results biologically? You don't have to be right, but don't be vague, and don't contradict your results.\n",
    "supporting": [
      "a1c_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}