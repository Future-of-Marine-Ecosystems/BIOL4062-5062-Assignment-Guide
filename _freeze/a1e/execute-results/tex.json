{
  "hash": "496e37ef58fdebd1bbaba2484ef7501a",
  "result": {
    "engine": "knitr",
    "markdown": "# **Assignment 1e:** Bayesian Data Analysis\n\nAssignment 1e is an introduction to Bayesian data analysis, using Bayesian generalized linear models.\n\nFor this tutorial, we'll be using `cuse.csv` .\n\n## Looking at the Data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load in data\ndata = read.csv('cuse.csv')\n\n# Look at the data structure\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  X   age education wantsMore notUsing using\n1 1   <25       low       yes       53     6\n2 2   <25       low        no       10     4\n3 3   <25      high       yes      212    52\n4 4   <25      high        no       50    10\n5 5 25-29       low       yes       60    14\n6 6 25-29       low        no       19    10\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16  6\n```\n\n\n:::\n:::\n\n\n\n\n\n\nOur data contains 16 observations of 5 variables - a two-column binomial matrix (notUsing and using) of the number of women using and not using birth control within 16 groups, and three categorical predictors - age, expressed as categories, education, and whether they want more children. The first column is a duplicate of our row names. We can get rid of that:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove column 1\ndata = data[,-1]\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    age education wantsMore notUsing using\n1   <25       low       yes       53     6\n2   <25       low        no       10     4\n3   <25      high       yes      212    52\n4   <25      high        no       50    10\n5 25-29       low       yes       60    14\n6 25-29       low        no       19    10\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Binomial GLM\n\nBinomial generalized linear models (logistic regression with a binary response variable and a logit link) are used to calculate the probability of a binomial response - in this case, whether someone is using or not using birth control. Binomial GLM responses can be fed in either as a true/false set, or as a two-column matrix of successes and failures. According to `?family`, we need to feed in the data with successes (whatever that means in each context) first and failures second. Let's create the matrix:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create response matrix\nresp = cbind(data$using, data$notUsing)\nhead(resp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    6   53\n[2,]    4   10\n[3,]   52  212\n[4,]   10   50\n[5,]   14   60\n[6,]   10   19\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn this case, all of our variables are categorical, and they are currently stored as characters:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check predictor classes\nclass(data$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"character\"\n```\n\n\n:::\n\n```{.r .cell-code}\nclass(data$education)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"character\"\n```\n\n\n:::\n\n```{.r .cell-code}\nclass(data$wantsMore)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"character\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThese should function fine as categorical variables. Let's make our GLM:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run GLM\nm1 = glm(resp ~ age + education + wantsMore, family = 'binomial', data = data)\nsummary(m1) # Summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = resp ~ age + education + wantsMore, family = \"binomial\", \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***\nage25-29       0.3894     0.1759   2.214  0.02681 *  \nage30-39       0.9086     0.1646   5.519 3.40e-08 ***\nage40-49       1.1892     0.2144   5.546 2.92e-08 ***\neducationlow  -0.3250     0.1240  -2.620  0.00879 ** \nwantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 165.772  on 15  degrees of freedom\nResidual deviance:  29.917  on 10  degrees of freedom\nAIC: 113.43\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn our summary we see we have 5 predictor categories with parameter estimates: different age bins (values representing the difference from the age \\<25 bin), low education (representing the difference from high), and wanting more kids (representing the difference from not wanting more kids). We can also see the values of our model coefficients, their standard errors, and the model AIC.\n\n## Making it Bayesian\n\nThe default GLM function is frequentist (that's why we have p-values). Now lets try a Bayesian approach:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stan\nlibrary(rstanarm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is rstanarm version 2.32.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  options(mc.cores = parallel::detectCores())\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(bayesplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is bayesplot version 1.14.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- Online documentation and vignettes at mc-stan.org/bayesplot\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- bayesplot theme set to bayesplot::theme_default()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n   * Does _not_ affect other ggplot2 plots\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n   * See ?bayesplot_theme_set for details on theme setting\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(shinystan)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: shiny\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nThis is shinystan version 2.6.0\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Run glm\nm2 = stan_glm(resp ~ age + education + wantsMore, family = 'binomial', data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.119 seconds (Warm-up)\nChain 1:                0.114 seconds (Sampling)\nChain 1:                0.233 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 2:                0.119 seconds (Sampling)\nChain 2:                0.228 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.119 seconds (Warm-up)\nChain 3:                0.127 seconds (Sampling)\nChain 3:                0.246 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 4:                0.101 seconds (Sampling)\nChain 4:                0.21 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m2) # Summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      resp ~ age + education + wantsMore\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   6\n\nEstimates:\n               mean   sd   10%   50%   90%\n(Intercept)  -0.8    0.2 -1.0  -0.8  -0.6 \nage25-29      0.4    0.2  0.2   0.4   0.6 \nage30-39      0.9    0.2  0.7   0.9   1.1 \nage40-49      1.2    0.2  0.9   1.2   1.5 \neducationlow -0.3    0.1 -0.5  -0.3  -0.2 \nwantsMoreyes -0.8    0.1 -1.0  -0.8  -0.7 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 31.7    1.6 29.8  31.7  33.8 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2292 \nage25-29      0.0  1.0  2450 \nage30-39      0.0  1.0  1905 \nage40-49      0.0  1.0  2441 \neducationlow  0.0  1.0  2984 \nwantsMoreyes  0.0  1.0  3592 \nmean_PPD      0.0  1.0  3976 \nlog-posterior 0.0  1.0  1871 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe `stan_glm` function automatically feeds our model into [Stan](https://mc-stan.org/), which is a Hamiltonian Markov Chain Monte Carlo (MCMC) sampler. Running `summary` on our model gives us some model diagnostics - all our Rhat values are 1 and all our n_eff values are well into the thousands, both of which are a good sign. We can also do some visual checks and tests:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Trace plot\nplot(m2, 'trace')\n```\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nThese are trace plots, which show us the parameter values selected for each iteration of the MCMC chain. We want these to look \"fuzzy\" - that indicates the sampler is exploring the full range of possible values. If these lines were flat, that would indicate the sampler got \"stuck\" and didn't sample the full posterior distributions. These look good.\n\nLets look at our posteriors:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot parameter values with uncertainties\nplot(m2, prob_outer = 0.95)\n```\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Plot posterior distributions\nplot(m2, 'mcmc_hist')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-8-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nThese plots both give us an idea of our parameter values and their posterior distributions. The former plot shows the median parameter estimates (circle), their 50% quantiles (dark blue box), and their 95% quantiles (thin blue line). The latter shows histograms of the posterior distributions of each of our parameters.\n\nWe can also pull out our coefficients and posteriors directly\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model coefficients\nm2$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)     age25-29     age30-39     age40-49 educationlow wantsMoreyes \n  -0.8017060    0.3798471    0.9064734    1.1916628   -0.3247363   -0.8412193 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Model posteriors\nposterior <- as.matrix(m2)\n\n# Plot model posteriors (95% quantile)\nplot_title <- ggtitle(\"Posterior distributions with medians and 95% credible intervals\")\n\nmcmc_areas(posterior, pars = names(m2$coefficients),\n           prob = 0.95) + plot_title\n```\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nHow would you interpret these plots? Note the asymmetry in the histograms, in contrast to typical frequentist approaches\n\n## Adding Priors\n\nLets try adding some priors:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run glm with priors\nm3 = stan_glm(resp ~ age + education + wantsMore, family = 'binomial', data = data,\n              prior = normal(location = c(0.2, 1.5, 2, -1, -0.25), # Normal priors, means\n                             scale = c(0.03, 0.03, 0.03, 0.03, 0.03))) # And standard deviations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.156 seconds (Warm-up)\nChain 1:                0.077 seconds (Sampling)\nChain 1:                0.233 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.122 seconds (Warm-up)\nChain 2:                0.071 seconds (Sampling)\nChain 2:                0.193 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 3:                0.076 seconds (Sampling)\nChain 3:                0.212 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.134 seconds (Warm-up)\nChain 4:                0.074 seconds (Sampling)\nChain 4:                0.208 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m3) # Summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      resp ~ age + education + wantsMore\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   6\n\nEstimates:\n               mean   sd   10%   50%   90%\n(Intercept)  -1.2    0.1 -1.3  -1.2  -1.1 \nage25-29      0.2    0.0  0.2   0.2   0.3 \nage30-39      1.5    0.0  1.4   1.5   1.5 \nage40-49      2.0    0.0  2.0   2.0   2.0 \neducationlow -1.0    0.0 -1.0  -1.0  -0.9 \nwantsMoreyes -0.3    0.0 -0.3  -0.3  -0.2 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 31.7    1.5 29.7  31.7  33.8 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  6574 \nage25-29      0.0  1.0  5638 \nage30-39      0.0  1.0  6199 \nage40-49      0.0  1.0  6705 \neducationlow  0.0  1.0  6761 \nwantsMoreyes  0.0  1.0  7387 \nmean_PPD      0.0  1.0  5118 \nlog-posterior 0.0  1.0  1973 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n\n\n\nLets look at our plots again:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Trace plot\nplot(m3, 'trace')\n```\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Plot parameter values with uncertainties\nplot(m3, prob_outer = 0.95)\n```\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-11-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Plot posterior distributions\nplot(m3, 'mcmc_hist')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-11-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Model coefficients\nm3$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)     age25-29     age30-39     age40-49 educationlow wantsMoreyes \n  -1.1997315    0.2194363    1.4816879    1.9952694   -0.9713091   -0.2810344 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Model posteriors\nposterior <- as.matrix(m3)\n\n# Plot model posteriors (95% quantile)\nplot_title <- ggtitle(\"Posterior distributions with medians and 95% credible intervals\")\n\nmcmc_areas(posterior, pars = names(m3$coefficients),\n           prob = 0.95) + plot_title\n```\n\n::: {.cell-output-display}\n![](a1e_files/figure-pdf/unnamed-chunk-11-4.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nWhat has changed versus the model without priors?\n\nYou can also look at all of your Stan model results using `shinystan` by running `launch_shinystan(model)`. Try it out on your end (it doesn't work in markdown)\n\n## Tips for your Assignment:\n\nSome things you may want to think about for your assignment:\n\n1.  How do the results of these three models differ? Why do they or don't they?\n\n2.  Do you interpret certain models as being more or less correct? Why or why not?\n\n3.  How would you interpret your statistical results biologically? You don't have to be right, but don't be vague, and don't contradict your results.\n",
    "supporting": [
      "a1e_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}