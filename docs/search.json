[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assignment 1 Guide",
    "section": "",
    "text": "Introduction\nWelcome to the assignment guide for BIOL4062/5062: Analysis of Biological Data.\nThis website is designed to walk you through the assignments for this class. It is a resource to help you figure out how to code for your assignments, and bring attention to key questions to ask yourself as you interpret your results, both statistically and biologically. Keep in mind that there are always different ways to get to the right answer with coding. The content here is not a monolith. You don’t need to follow it if you don’t want to (more in Assignment Guidelines), but make sure what you are doing is clear and sufficiently analogous to this guide, else you lose marks for being unclear, or running the wrong analyses."
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "Assignment 1 Guide",
    "section": "Getting Help",
    "text": "Getting Help\nDon’t suffer in silence! If you need help on the assignments, there are multiple options available to you:\n\nAssignment Drop-In Sessions:\n\nTA-run in-person help sessions the week before each assignment is due.\nOptional, but recommended\n\nEven if you don’t have questions, you may benefit from hearing the questions of others\n\n\nBrightSpace Discussion Board:\n\nFeel free to ask questions on the BrightSpace discussion board, or peruse questions already asked\n\nYou may find your answer without even asking!\n\nMake sure to start your question with the assignment number\n\nEmail\n\nFeel free to ask questions, or set up an appointment\nDirect Assignment 1 questions to the TA, and Assignment 2 and/or class administration (e.g. extension requests) to the instructors\n\n\nWithout further ado, let’s get into it!"
  },
  {
    "objectID": "guidelines.html#general-advice",
    "href": "guidelines.html#general-advice",
    "title": "Assignment Guidelines",
    "section": "General Advice",
    "text": "General Advice\n1. Read the grading rubric!\n\nIt is quite objective. There is little latitude for part marks if you are missing parts that you need.\n\n2. You don’t need to tell me the statistical theory or background (unless it’s relevant to your answers)\n\nAll you have to do is answer the questions in the assignment. Anything you write outside of that is just eating up your page limit.\n\n3. Put your biological interpretations together at the end\n\nYour interpretations are more likely to make sense and easier to mark if you put them at the end, and include all of your results together in them rather than inserting them throughout piecemeal.\nAlso, make sure your biological interpretations are consistent with your data/results! They don’t have to be correct, but they have to match your data.\n\n4. Make sure your figures are readable\n\nWe can’t tell if your interpretation of your figures is correct if I can’t interpret your figures.\nAll text on figures should be readable.\nIf you use color, make sure that the colors you use are clearly distinguishable.\n\n5. You’re not alone!\n\nIf you have questions, come to the drop-in sessions, read the discussion boards, or email the TA to ask questions or set up a meeting if those don’t work for you.\nDo the first two even if you don’t have questions: You may find the answer to questions you didn’t know you had.\nYou’re also welcome to ask questions after an assignment if you want to know why you were graded the way you were, or if you have questions about the comments provided or what you may have done wrong.\n\n6. Ask if you need an extension\n\nWe’re pretty reasonable."
  },
  {
    "objectID": "guidelines.html#submission-formatting",
    "href": "guidelines.html#submission-formatting",
    "title": "Assignment Guidelines",
    "section": "Submission Formatting",
    "text": "Submission Formatting\nHere are some guidelines on how to submit your assignment to make my life easier. You won’t lose marks for not following them, but I would greatly appreciate it if you did.\n1. Hand in your assignment in 3 parts:\n\nYour assignment text\n\n\nAll assignment 1s have a 2 page limit. Put all your text first, with figures and tables together separately at the end. It is easier for me to tell how many pages you wrote this way. You’re not going to lose marks if you’re slightly over (this is not a writing class), if you’re going to lose marks for writing too many pages we’re going to be able to tell anyways.\nShould be a doc or PDF file so it can be opened in BrightSpace. Doesn’t matter if it’s produced through word, markdown, etc, as long as it’s in one of those two formats.\n\nb) Your script, submitted as a .txt file\n\nSubmitting as a .txt file allows us to open the script in BrightSpace rather than having to download it if it’s a .R file. It is much more difficult to run your script should we need to if you paste it in your assignment document.\n\nc) The data you read into your script\n\nThis is just to make it easy to run your script if there is a mistake.\n\nNote: If you do your assignments in markdown, you can combine a) and b)\n2. Don’t put your name on your assignments, in your scripts, or in any of your file names\n\nThe BrightSpace system is anonymous so your assignments are marked blind. That doesn’t work if you write your name.\nDelete your file paths in your script for submission if they have your name in them.\nYou do need to put your B0 number, data code number, and whether you’re a graduate student or an undergraduate student.\n\n3. Follow the assignment guide\n\nYou don’t have to follow this guide to get full marks on the assignments (as always, there are many correct answers when it comes to coding). That said, it’s easier to follow what you’re doing if you’re doing the same thing as everyone else.\nIt’s OK to do your own thing, but if you make a mistake, its going to be much harder to help you out, and it’s going to take significantly more effort to mark."
  },
  {
    "objectID": "a1a.html#looking-at-the-data",
    "href": "a1a.html#looking-at-the-data",
    "title": "1  Assignment 1a: Principal Components Analysis",
    "section": "1.1 Looking at the Data",
    "text": "1.1 Looking at the Data\nWith any data analysis, step 1 is always to look at your data:\n\n# Load in data\ndata = read.csv('fishcatch.csv')\n\n# View data structure\nhead(data)\n\n  Hauls mackerel bluefin sardine squid limpet\n1     1    1.851   55.60   0.058  6.00 0.0004\n2     2    1.925    1.20   0.252  0.08 0.0027\n3     3    2.506    1.56   0.133  0.06 0.0015\n4     4    1.537   30.00   0.064  9.35 0.0013\n5     5    1.795    0.04   0.086  4.70 0.0022\n6     6    3.371   45.00   0.078  7.66 0.0006\n\ndim(data)\n\n[1] 25  6\n\n\nOur data is a 25 row, 6 column data frame, describing catch of 5 different fisheries species (columns 2-6) caught across 25 hauls (column 1). We want to know if certain species are associated with each other. Lets look a little deeper at the data:\n\n# Generate boxplots\nboxplot(data[,-1]) # Exclude haul\n\n\n\n# look at data distribution\n# par(mfrow = c(3,2)) # 1 column 5 row grid plot\nhist(data$mackerel, breaks = 10)\n\n\n\nhist(data$bluefin, breaks = 10)\n\n\n\nhist(data$sardine, breaks = 10)\n\n\n\nhist(data$squid, breaks = 10)\n\n\n\nhist(data$limpet, breaks = 10)\n\n\n\n\nA few things are immediately obvious from looking at our data:\n1. There are some large outliers\n2. The data scales vary greatly across species\n3. The species all have relatively different distributions, none of which look normal.\nAre these issues? How do we fix them?"
  },
  {
    "objectID": "a1a.html#transformations",
    "href": "a1a.html#transformations",
    "title": "1  Assignment 1a: Principal Components Analysis",
    "section": "1.2 Transformations",
    "text": "1.2 Transformations\nLook back at the PCA lecture. What are potential problems with PCA?\n1. Covariance Matrix PCA requires data to be in the same units\n2. Normality is desirable, but not essential\n3. Precision is desireable, but not essential\n4. Many zeroes in the data\nWe can fix issue 1 by logging our data:\n\n# Create a new data object so we can log the data\ndata_log = data\n\n# Log data\ndata_log[,-1] = log(data_log[,-1]) # Remember to exclude haul\n\nNow that we’ve transformed the data, let’s check for normality again:\n\n# Generate histograms\n# par(mfrow = c(3,2)) # 1 column 5 row grid plot\nhist(data_log$mackerel, breaks = 10)\n\n\n\nhist(data_log$bluefin, breaks = 10)\n\n\n\nhist(data_log$sardine, breaks = 10)\n\n\n\nhist(data_log$squid, breaks = 10)\n\n\n\nhist(data_log$limpet, breaks = 10)\n\n\n\n\nThese look much better. We can also confirm this statistically:\n\n# Generate histograms\nshapiro.test(data_log$mackerel)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_log$mackerel\nW = 0.9425, p-value = 0.1691\n\nshapiro.test(data_log$bluefin)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_log$bluefin\nW = 0.98186, p-value = 0.9193\n\nshapiro.test(data_log$sardine)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_log$sardine\nW = 0.94113, p-value = 0.1572\n\nshapiro.test(data_log$squid)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_log$squid\nW = 0.96226, p-value = 0.4613\n\nshapiro.test(data_log$limpet)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_log$limpet\nW = 0.96437, p-value = 0.5082\n\n\nAll 5 species fail to reject the null hypothesis that the data are normally distributed. Logging the data also helps deal with the outliers:\n\n# Generate boxplots\nboxplot(data_log[,-1])\n\n\n\n\nNote that we can only log the data if there are no zeroes:\n\n# Generate test data\ndata_test = data; data_test[1,6] = 0 # Change the first limpet value to 0\n\n# Try to log the data\ndata_test[1,] # Print first row\n\n  Hauls mackerel bluefin sardine squid limpet\n1     1    1.851    55.6   0.058     6      0\n\nlog(data_test[,-1])[1,] # Print logs of the first row\n\n  mackerel  bluefin   sardine    squid limpet\n1 0.615726 4.018183 -2.847312 1.791759   -Inf\n\n\nlog(0) returnes negative infinity. That’s going to be a problem later in our analysis. We can fix that by adding a small increment before taking the log. Keep in mind though that each species has a different magnitude in this dataset, and adding an inappropriate increment could cause us trouble later:\n\n# Test boxplots of different increments\nboxplot(log(data_test$limpet), # Warning because of -Inf\n        log(data_test$limpet + 1),\n        log(data_test$limpet + 0.001),\n        log(data_test$limpet + 0.000000001))\n\nWarning in bplt(at[i], wid = width[i], stats = z$stats[, i], out =\nz$out[z$group == : Outlier (-Inf) in boxplot 1 is not drawn\n\n\n\n\n\nIf the increment is too big, we eliminate the variance in our data. If the increment is to small, we create an outlier."
  },
  {
    "objectID": "a1a.html#running-pca",
    "href": "a1a.html#running-pca",
    "title": "1  Assignment 1a: Principal Components Analysis",
    "section": "1.3 Running PCA",
    "text": "1.3 Running PCA\nNow that we’ve checked and transformed our data, we’re ready to run PCA. There are two kinds of PCA: We can run PCA on the Covariance Matrix, or the Correlation Matrix.\n\n1.3.1 Covariance Matrix\nWe can run PCA on the covariance matrix as follows:\n\n# Run PCA - Covariance\npca_1 = princomp(data_log[,-1]) # We don't want haul in our PCA!\nsummary(pca_1)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4      Comp.5\nStandard deviation     2.8055354 1.3857803 1.3351790 0.55247629 0.182937780\nProportion of Variance 0.6607195 0.1612035 0.1496458 0.02562199 0.002809263\nCumulative Proportion  0.6607195 0.8219229 0.9715687 0.99719074 1.000000000\n\n\nRunning a summary on our PCA gives us the standard deviation of each principal component, the proportion of variance explained by each principal component, and the cumulative variance explained as we add each component. Here, we see the first principal component explains 66% of the variance. The second explains 16%, which adds up to 82% with the first component, and so on up to component 5. We can visualize the cumulative variance explained with a scree plot:\n\n# Generate scree plot\nplot(pca_1, type = 'l') # Scree is built into the plot for PCA\n\n\n\n\nWe see most of the variance is explained by component 1, then a similar lesser amount is explained by 2 and 3, followed by another drop to 4 and 5.\n\n# Print loadings\nprint(loadings(pca_1),cutoff=0.00) #all loadings!\n\n\nLoadings:\n         Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nmackerel  0.018  0.294  0.047  0.527  0.796\nbluefin  -0.654  0.136  0.739 -0.089 -0.020\nsardine   0.060  0.626 -0.015  0.520 -0.577\nsquid    -0.745  0.049 -0.664  0.029  0.018\nlimpet    0.116  0.707 -0.102 -0.665  0.183\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n\n\nThe PCA loadings are the correlations between the variables and each component. Here, we see bluefin and squid are strongly negatively correlated with component 1, while mackerel, sardine, and limpet are weakly positively correlated with component 1. We can continue this type of interpretation through the other components as well.\nOur PCA object also contains the PCA scores for each individual data point:\n\n# Print PCA scores\nhead(pca_1$scores)\n\n        Comp.1     Comp.2     Comp.3     Comp.4      Comp.5\n[1,] -3.551007 -2.2507718  0.6725187 -0.1223707 -0.06754585\n[2,]  2.484116 -0.6980669  0.4886052 -0.3932653 -0.53609582\n[3,]  2.425206 -1.4149241  0.9556963 -0.2274184 -0.07560834\n[4,] -3.339469 -1.4723306 -0.2089590 -0.8854067 -0.03598265\n[5,]  1.580988 -1.8002844 -4.6958830 -0.4313924  0.13590020\n[6,] -3.519487 -1.6187995  0.3362833  0.1040827  0.32134755\n\n\nScores are the value of each data point on each principal component. Lets try plotting them:\n\n# Plot scores - components 1 and 2\nplot(pca_1$scores[,1], # Scores on component 1\n     pca_1$scores[,2], # Scores on component 3\n     pch=16, # Point 16 (colored circle)\n     xlab=\"1st principal component\",ylab=\"2nd principal component\",main=\"Scores plot\") # Axis and plot labels\n\n\n\n\nThis generates a scatterplot showing us the value of each data point in principal components 1 (x) and 2 (y). Now lets add on the loadings:\n\n# Plot scores - components 1 and 2\nplot(pca_1$scores[,1], # Scores on component 1\n     pca_1$scores[,2], # Scores on component 3\n     pch=16, # Point 16 (colored circle)\n     xlab=\"1st principal component\",ylab=\"2nd principal component\",main=\"Scores plot\") # Axis and plot labels\n\n# Add loadings to plot\narrows(0,0, # Draw arrows from zero\n       pca_1$loadings[,1], # Draw to PC1 loading in X\n       pca_1$loadings[,2], # Draw to PC2 loading in Y\n       col=\"black\", length = 0.1) # Arrow color and arrowhead length\ntext(pca_1$loadings[,1],pca_1$loadings[,2],names(data_log[,-1]),cex=1.0 ,col=\"black\") # Add text labels for each variable\n\n\n\n\nThe arrows are a little small, so let’s add a scaling factor:\n\n# Plot scores - components 1 and 2\nplot(pca_1$scores[,1], # Scores on component 1\n     pca_1$scores[,2], # Scores on component 3\n     pch=16, # Point 16 (colored circle)\n     xlab=\"1st principal component\",ylab=\"2nd principal component\",main=\"Scores plot\") # Axis and plot labels\n\n# Add loadings to plot\nsf = 3 # Scaling factor\nsft = 3.2 # Scaling factor for text\narrows(0,0, # Draw arrows from zero\n       pca_1$loadings[,1]*sf, # Draw to PC1 * scaling factor loading in X\n       pca_1$loadings[,2]*sf, # Draw to PC2 * scaling factor loading in Y\n       col=\"black\", length = 0.1) # Arrow color and arrowhead length\ntext(pca_1$loadings[,1]*sft,pca_1$loadings[,2]*sft, names(data_log[,-1]), cex=1.0, col=\"black\") # Add text labels for each variable\n\n\n\n\nWhat about the haul number? Does that have an effect? Let’s try adding that on as well:\n\n# Create a color palette\ncolfunc = colorRampPalette(c('orangered1', 'turquoise2'))\n\n# Plot scores - components 1 and 2\nplot(pca_1$scores[,1], # Scores on component 1\n     pca_1$scores[,2], # Scores on component 3\n     pch=16, # Point 16 (colored circle)\n     col = colfunc(nrow(pca_1$scores)), # Color points by haul using our color palette\n     xlab=\"1st principal component\",ylab=\"2nd principal component\",main=\"Scores plot\") # Axis and plot labels\n\n# Add loadings to plot\nsf = 3 # Scaling factor\nsft = 3.2 # Scaling factor for text\narrows(0,0, # Draw arrows from zero\n       pca_1$loadings[,1]*sf, # Draw to PC1 * scaling factor loading in X\n       pca_1$loadings[,2]*sf, # Draw to PC2 * scaling factor loading in Y\n       col=\"black\", length = 0.1) # Arrow color and arrowhead length\ntext(pca_1$loadings[,1]*sft,pca_1$loadings[,2]*sft, names(data_log[,-1]), cex=1.0, col=\"black\") # Add text labels for each variable\n\n\n\n\nSince we used color for haul, we need to add a legend:\n\n# Set plot layout\nlayout(matrix(1:2,ncol=2), # 1 row, 2 columns\n       width = c(2,1), # Width\n       height = c(1,1)) # Height\n\n# Create a color palette\ncolfunc = colorRampPalette(c('orangered1', 'turquoise2'))\n\n# Plot scores - components 1 and 2\nplot(pca_1$scores[,1], # Scores on component 1\n     pca_1$scores[,2], # Scores on component 3\n     pch=16, # Point 16 (colored circle)\n     col = colfunc(nrow(pca_1$scores)), # Color points by haul using our color palette\n     xlab=\"1st principal component\",ylab=\"2nd principal component\",main=\"Scores plot\") # Axis and plot labels\n\n# Add loadings to plot\nsf = 3 # Scaling factor\nsft = 3.2 # Scaling factor for text\narrows(0,0, # Draw arrows from zero\n       pca_1$loadings[,1]*sf, # Draw to PC1 * scaling factor loading in X\n       pca_1$loadings[,2]*sf, # Draw to PC2 * scaling factor loading in Y\n       col=\"black\", length = 0.1) # Arrow color and arrowhead length\ntext(pca_1$loadings[,1]*sft,pca_1$loadings[,2]*sft, names(data_log[,-1]), cex=1.0, col=\"black\") # Add text labels for each variable\n\n# Generate legend\nlegend_image &lt;- as.raster(matrix(colfunc(nrow(pca_1$scores)), ncol=1))\nplot(c(0,2),c(0,1),type = 'n', axes = F,xlab = '', ylab = '', main = 'Haul')\ntext(x=1.5, y =seq(0,1,l=5), labels = seq(1,25,l=5))\nrasterImage(legend_image, 0, 0, 1,1)\n\n\n\n\nNow we have a completed scores plot with loadings arrows. How would you interpret this plot?\n\n\n1.3.2 Correlation Matrix\nNow let’s try the correlation matrix. The correlation matrix performs the same analysis, but on standardized data. The princomp() function does this for us if we set cor = T:\n\n# Run PCA - Correlation\npca_2 = princomp(data_log[,-1], cor = T)\nsummary(pca_2)\n\nImportance of components:\n                         Comp.1    Comp.2     Comp.3     Comp.4     Comp.5\nStandard deviation     1.595782 1.2503536 0.70708572 0.57220519 0.25041296\nProportion of Variance 0.509304 0.3126768 0.09999404 0.06548376 0.01254133\nCumulative Proportion  0.509304 0.8219809 0.92197491 0.98745867 1.00000000\n\n# In case you don't believe me, heres the covariance matrix if we pre-standardize the data\npca_test = princomp(scale(data_log[-1]))\nsummary(pca_test)\n\nImportance of components:\n                         Comp.1    Comp.2     Comp.3     Comp.4     Comp.5\nStandard deviation     1.563541 1.2250914 0.69279969 0.56064429 0.24535359\nProportion of Variance 0.509304 0.3126768 0.09999404 0.06548376 0.01254133\nCumulative Proportion  0.509304 0.8219809 0.92197491 0.98745867 1.00000000\n\n\nNow we can go through the same pattern of analyses as we did for covariance:\n\n# Generate scree plot\nplot(pca_2, type = 'l') # Scree is built into the plot for PCA\n\n\n\n# Print loadings\nprint(loadings(pca_2),cutoff=0.00) #all loadings!\n\n\nLoadings:\n         Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nmackerel  0.524  0.272  0.527  0.297  0.535\nbluefin  -0.198  0.682  0.264 -0.651 -0.050\nsardine   0.591  0.209  0.025  0.109 -0.771\nsquid    -0.233  0.645 -0.472  0.550  0.059\nlimpet    0.532  0.036 -0.655 -0.416  0.338\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n\n# Set plot layout\nlayout(matrix(1:2,ncol=2), # 1 row, 2 columns\n       width = c(2,1), # Width\n       height = c(1,1)) # Height\n\n# Create a color palette\ncolfunc = colorRampPalette(c('orangered1', 'turquoise2'))\n\n# Plot scores - components 1 and 2\nplot(pca_2$scores[,1], # Scores on component 1\n     pca_2$scores[,2], # Scores on component 3\n     pch=16, # Point 16 (colored circle)\n     col = colfunc(nrow(pca_2$scores)), # Color points by haul using our color palette\n     xlab=\"1st principal component\",ylab=\"2nd principal component\",main=\"Scores plot\") # Axis and plot labels\n\n# Add loadings to plot\nsf = 3 # Scaling factor\nsft = 3.2 # Scaling factor for text\narrows(0,0, # Draw arrows from zero\n       pca_2$loadings[,1]*sf, # Draw to PC1 * scaling factor loading in X\n       pca_2$loadings[,2]*sf, # Draw to PC2 * scaling factor loading in Y\n       col=\"black\", length = 0.1) # Arrow color and arrowhead length\ntext(pca_2$loadings[,1]*sft,pca_2$loadings[,2]*sft, names(data_log[,-1]), cex=1.0, col=\"black\") # Add text labels for each variable\n\n# Generate legend\nlegend_image &lt;- as.raster(matrix(colfunc(nrow(pca_2$scores)), ncol=1))\nplot(c(0,2),c(0,1),type = 'n', axes = F,xlab = '', ylab = '', main = 'Haul')\ntext(x=1.5, y =seq(0,1,l=5), labels = seq(1,25,l=5))\nrasterImage(legend_image, 0, 0, 1,1)\n\n\n\n\nHow would you interpret this plot? Does it differ from the covariance plot?\n\n\n1.3.3 Alternative Methods\nThere are a few other ways you can generate, and/or plot your PCAs if you prefer.\n\n1.3.3.1 Biplot\n\n# Exploring biplot\nbiplot(pca_1) # Covariance\n\n\n\nbiplot(pca_2) # Correlation\n\n\n\n\n\n\n1.3.3.2 ggplot\n\nlibrary(ggplot2)\n\n# ggplot version - Covariance\n\n# turn PCA scores into data frame\npca_1_plot = data.frame(Haul = data_log$Haul, pca_1$scores) \n\n# Turn PCA loadings into data frame (This gets a little complicated)\npca_1_loadings = as.data.frame(matrix(as.numeric(pca_1$loadings), \n                                      dim(pca_1$loadings)[1], dim(pca_1$loadings)[2]))\ncolnames(pca_1_loadings) = colnames(pca_1_plot)[-1]\n\n# Plot\nggplot(pca_1_plot, aes(x = Comp.1, y = Comp.2, color = Haul)) +\n  \n  # Scores\n  geom_point() + scale_colour_distiller(palette = 15) + \n  \n  # Loadings\n  geom_segment(data = pca_1_loadings, aes(x = 0, y = 0,xend = Comp.1 , yend = Comp.2), \n    arrow = arrow(length = unit(0.3, \"cm\"), type = \"open\", angle = 25), \n    linewidth = 1, color = \"darkblue\") + \n  \n  # Labels\n  geom_text(data = pca_1_loadings, color = 'darkblue', nudge_x = 0.2, nudge_y = 0.2, # Labels\n                aes(x = Comp.1, y = Comp.2, label = colnames(data_log)[-1]))\n\n\n\n# ggplot version - Correlation\n\n# turn PCA scores into data frame\npca_2_plot = data.frame(Haul = data_log$Haul, pca_2$scores) \n\n# Turn PCA loadings into data frame\npca_2_loadings = as.data.frame(matrix(as.numeric(pca_2$loadings), \n                                      dim(pca_2$loadings)[1], dim(pca_2$loadings)[2]))\ncolnames(pca_2_loadings) = colnames(pca_2_plot)[-1]\n\n# Plot\nggplot(pca_2_plot, aes(x = Comp.1, y = Comp.2, color = Haul)) +\n  \n  # Scores\n  geom_point() + scale_colour_distiller(palette = 15) + \n  \n  # Loadings\n  geom_segment(data = pca_2_loadings, aes(x = 0, y = 0,xend = Comp.1 , yend = Comp.2), \n               arrow = arrow(length = unit(0.3, \"cm\"), type = \"open\", angle = 25), \n               linewidth = 1, color = \"darkblue\") + \n  \n  # Labels\n  geom_text(data = pca_2_loadings, color = 'darkblue', nudge_x = 0.2, nudge_y = 0.2, # Labels\n            aes(x = Comp.1, y = Comp.2, label = colnames(data_log)[-1]))\n\n\n\n\nYou can also run PCA using the prcomp() function instead of princomp(), setting scale = T if you want the correlation matrix. You can then use autoplot() with the ggfortify package to plot the results.\n\n# ggplot v2\nlibrary(ggfortify)\n\n# Run PCA - Covariance\npca_1a = prcomp(data_log[,-1])\n\n# Run autoplot\nautoplot(pca_1a, data = data_log, color = 'Hauls', loadings = T, loadings.label = T)\n\n\n\n# Run PCA - Correlation\npca_2a = prcomp(data_log[,-1], scale = T)\n\n# Run autoplot\nautoplot(pca_2a, data = data_log, color = 'Hauls', loadings = T, loadings.label = T)"
  },
  {
    "objectID": "a1a.html#varimax-rotation-optional",
    "href": "a1a.html#varimax-rotation-optional",
    "title": "1  Assignment 1a: Principal Components Analysis",
    "section": "1.4 Varimax Rotation (Optional)",
    "text": "1.4 Varimax Rotation (Optional)\nVarimax rotation attempts to improve the interpretability of PCA results by lining up loadings with the axes. This can be useful, particularly with large numbers of variables.\n\n# Scaling factors\nsf = 2.5\nsft = 2.8\n\n# Varimax rotation - Covariance\nv1 = varimax(pca_1$loadings[,1:2])\nv1_scores = pca_1$scores[,1:2]%*%v1$rotmat\n\n# Plot scores - components 1 and 2\nplot(v1_scores[,1],v1_scores[,2],pch=15, col = colfunc(nrow(v1_scores)),\n     xlab=\"1st varimax component\",ylab=\"2nd varimax component\",main=\"varimax scores plot\")\n\n# Add loadings\narrows(0,0,v1$loadings[,1]*sf,v1$loadings[,2]*sf,col=\"black\")\ntext(v1$loadings[,1]*sft,v1$loadings[,2]*sft,names(data_log[,-1]),asp=1,cex=1.0 ,col=\"black\")\n\n\n\n# Varimax rotation - Correlation\nv2 = varimax(pca_2$loadings[,1:2])\nv2_scores = pca_2$scores[,1:2]%*%v2$rotmat\n\n# Plot scores - components 1 and 2\nplot(v2_scores[,1],v2_scores[,2],pch=15, col = colfunc(nrow(v2_scores)),\n     xlab=\"1st varimax component\",ylab=\"2nd varimax component\",main=\"varimax scores plot\")\n\n# Add loadings\narrows(0,0,v2$loadings[,1]*sf,v2$loadings[,2]*sf,col=\"black\")\ntext(v2$loadings[,1]*sft,v2$loadings[,2]*sft,names(data_log[,-1]),asp=1,cex=1.0 ,col=\"black\")\n\n\n\n\nNote that it’s pretty hard to tell the hauls apart using this color scale. Make sure your plots are always clear and readable."
  },
  {
    "objectID": "a1a.html#tips-for-your-assignment",
    "href": "a1a.html#tips-for-your-assignment",
    "title": "1  Assignment 1a: Principal Components Analysis",
    "section": "1.5 Tips for your assignment:",
    "text": "1.5 Tips for your assignment:\nSome things you may want to think about for your assignment:\n1. Do your covariance and correlation plots differ? Do you think one is better suited to answering your research question? Why? Is your answer conceptual, or does it have to do with the results? Both?\n2. How would you quantitatively examine the effect of haul on the PCA scores above? Is it associated with any of the principal components?\n3. How would you interpret your statistical results biologically? You don’t have to be right, but don’t be vague, and don’t contradict your results."
  },
  {
    "objectID": "a1b.html#looking-at-the-data",
    "href": "a1b.html#looking-at-the-data",
    "title": "2  Assignment 1b: Linear Discriminant Analysis",
    "section": "2.1 Looking at the data",
    "text": "2.1 Looking at the data\n\n# Load in data\nsnake = read.csv('snake.csv')\n\n# Look at data\nhead(snake)\n\n  Species   M1   M2   M3   M4   M5   M6\n1     A   41.6  6.7  8.2 12.2 24.7 27.0\n2     A   40.2  8.5  9.2 15.5 27.1 30.3\n3     A   40.4 12.6 14.2 19.6 46.9 26.8\n4     A   26.4  9.0  8.6 14.0 37.6 32.2\n5     A   34.4  7.0 12.1 11.1 31.0 35.8\n6     A   38.8  8.2 10.2 12.4 42.2 33.6\n\ndim(snake)\n\n[1] 35  7\n\n\nOur data is a 35 row, 7 column data frame. The first column identifies the species of snake (A or B). The other columns are morphological measurements of each individual snake. We want to know if we can use the morphological measurements of the snakes to determine their species. Let’s keep examining the data:\n\n# Make a boxplot\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Convert the data to long format so we can use ggplot\nsnake_long = pivot_longer(snake, # Enter data\n                          colnames(snake)[-1], # Pivot all columns except species\n                          names_to = 'Measurement', values_to = 'Value') # Feed labels to new data frame\n\n# Lets take a look at the new data frame\nhead(snake_long)\n\n# A tibble: 6 × 3\n  Species  Measurement Value\n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;\n1 \"   A  \" M1           41.6\n2 \"   A  \" M2            6.7\n3 \"   A  \" M3            8.2\n4 \"   A  \" M4           12.2\n5 \"   A  \" M5           24.7\n6 \"   A  \" M6           27  \n\n# We've converted from wide format to long format,\n# now all the data values are contained in a single column\n# which is described by a metadata column\n\n# You can also do this with melt from reshape2\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\nhead(melt(snake))\n\nUsing Species as id variables\n\n\n  Species variable value\n1     A         M1  41.6\n2     A         M1  40.2\n3     A         M1  40.4\n4     A         M1  26.4\n5     A         M1  34.4\n6     A         M1  38.8\n\n# Let's make a boxplot\nggplot(snake_long, aes(x = Measurement, y = Value, fill = Species)) +\n  geom_boxplot() + theme_classic()\n\n\n\n# We can do this in R base plot too\nboxplot(Value ~ Species*Measurement, # Plot value by species and measurement\n        data = snake_long, col = c('coral', 'turquoise2'), # Color by species\n        xaxt = 'n', xlab = 'Measurement') # Remove and label x axis\nlegend('topleft', legend = c('Species A', 'Species B'), fill = c('coral', 'turquoise2')) # Add a legend\naxis(1, at = seq(1.5,11.5,2), labels = colnames(snake)[-1]) # Add x axis back in with appropriate labels\n\n\n\n\nSome of our measurements are very similar across species, and others are quite different. Do they differ statistically as a whole?"
  },
  {
    "objectID": "a1b.html#manova",
    "href": "a1b.html#manova",
    "title": "2  Assignment 1b: Linear Discriminant Analysis",
    "section": "2.2 MANOVA",
    "text": "2.2 MANOVA\nThe purpose of LDA is to try to discriminate our snakes into species based on their measurements. However, that only makes sense to do if our two species of snake actually differ across the measurements. Our first step then is to discern whether our snake species differ as a multivariate whole. We’ll do this using a MANOVA.\n\n# Run MANOVA\nsm = manova(cbind(M1,M2,M3,M4,M5,M6) ~ Species, data = snake)\nsummary(sm, test = 'Hotelling')\n\n          Df Hotelling-Lawley approx F num Df den Df   Pr(&gt;F)    \nSpecies    1           1.2263   5.7229      6     28 0.000552 ***\nResiduals 33                                                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(sm, test = 'Wilks')\n\n          Df   Wilks approx F num Df den Df   Pr(&gt;F)    \nSpecies    1 0.44917   5.7229      6     28 0.000552 ***\nResiduals 33                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBy both the Hotelling’s and Wilks’ tests, our MANOVA is significant, indicating the snake species vary as a multivariate whole.\nWhat about our assumptions though? Our MANOVA assumptions are normality, linearity, and homogeneity of covariances. You’ve been told to assume the latter, so let’s skip that one.\n\n# Testing normality\nlibrary(mvnormtest)\nmshapiro.test(t(sm$residuals))\n\n\n    Shapiro-Wilk normality test\n\ndata:  Z\nW = 0.91571, p-value = 0.01075\n\n\nUh oh, the residuals are significantly non-normal. Let’s take a look at them visually:\n\n# Residual histogram\nhist(t(sm$residuals), breaks = 20)\n\n\n\n\nVisually, our residuals actually look quite close to normal. There may be some slight skew, or outliers that are forcing our residuals to statistical non-normality. We might be able to fix this by removing multivariate outliers, or by transforming some of our data (feel free to play around with these ideas!), but based on the shape of our residuals, it is unlikely that our model is fatally biased, and we may end up doing more harm than good. Based on this, we can conclude that our two species have significantly different morphometries given the measurements provided."
  },
  {
    "objectID": "a1b.html#linear-discriminant-analysis",
    "href": "a1b.html#linear-discriminant-analysis",
    "title": "2  Assignment 1b: Linear Discriminant Analysis",
    "section": "2.3 Linear Discriminant Analysis",
    "text": "2.3 Linear Discriminant Analysis\nNow that we’ve confirmed our species differ as a multivariate whole, we can try to use LDA to build a model to predict which species each snake belongs to based on its measurements.\n\n# LDA\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nldaf1 &lt;- lda(Species ~ M1+M2+M3+M4+M5+M6, snake)\nldaf1\n\nCall:\nlda(Species ~ M1 + M2 + M3 + M4 + M5 + M6, data = snake)\n\nPrior probabilities of groups:\n      A         B   \n0.2857143 0.7142857 \n\nGroup means:\n           M1     M2    M3    M4     M5     M6\n   A   32.700  9.410 10.16 15.54 35.290 28.950\n   B   31.496 12.128 10.18 16.78 47.356 21.752\n\nCoefficients of linear discriminants:\n           LD1\nM1  0.01428023\nM2  0.29104494\nM3 -0.07327616\nM4 -0.05544769\nM5  0.03629586\nM6 -0.17208517\n\n\nRunning our LDA object tells us the prior probabilities used for each species (the proportion of each species in the data), the group means for each measure on each species, and the linear discriminant (LD1) for each measure. We can then plot the LD1 value for each individual:\n\n# Plot discriminant function analysis\n\n# Create a data frame to plot\nldaf_plot = cbind(snake, # Data\n                  predict(ldaf1)$x, # LD1 value for each individual given its measurements\n                  index = seq(1,nrow(snake), 1)) # Row/Individual number\n\n# Plot\nplot(LD1 ~ index, data = ldaf_plot, col = as.factor(snake$Species), pch = 16)\nlegend('topleft', legend = c('A', 'B'), col = c(1, 2), pch = 16) # Add legend\n\n\n\n\nHere we can see higher LD1 values are associated with species B, while lower LD1 values are associated with species A. This is just based on model fit however; how do we know we aren’t overfitting? One way to avoid overfitting is by jackknifing (AKA leave-one-out cross validation in this context). This method runs the model once without each point in the dataset, then calculates the posterior probability that the left out point belongs to each species. Let’s try it out:\n\n# LDA 2, CV = T\nldaf2 = lda(Species ~ M1+M2+M3+M4+M5+M6, snake, CV = T)\n\n# Gather posteriors\nas.data.frame(cbind(ldaf2$posterior, # Pull posteriors from ldaf2\n                    ResultantSpp=as.character(ldaf2$class))) # Pull predicted species (i.e. species with the higher posterior probability)\n\n                    A                    B   ResultantSpp\n1     0.897801237948675    0.102198762051325          A  \n2     0.957033498274347   0.0429665017256531          A  \n3   0.00486396795570833    0.995136032044292          B  \n4     0.939579607872302   0.0604203921276979          A  \n5     0.999020574119129 0.000979425880871094          A  \n6     0.958283942083953    0.041716057916047          A  \n7     0.859914694048175    0.140085305951825          A  \n8    0.0790276689479006      0.9209723310521          B  \n9     0.250711809994116    0.749288190005884          B  \n10    0.277233534989757    0.722766465010243          B  \n11   0.0654339037846645    0.934566096215335          B  \n12 8.13045175684633e-05    0.999918695482432          B  \n13  0.00857331675606214    0.991426683243938          B  \n14    0.119793120831736    0.880206879168264          B  \n15    0.868897347918874    0.131102652081126          A  \n16    0.291404395123413    0.708595604876587          B  \n17    0.580893601645515    0.419106398354485          A  \n18    0.407526292222816    0.592473707777184          B  \n19    0.097139347240766    0.902860652759234          B  \n20   0.0629455676122022    0.937054432387798          B  \n21   0.0262176442553781    0.973782355744622          B  \n22   0.0110464412594654    0.988953558740534          B  \n23     0.37967670676917     0.62032329323083          B  \n24  0.00300786068222619    0.996992139317774          B  \n25   0.0331152011340242    0.966884798865976          B  \n26  0.00270158005931187    0.997298419940688          B  \n27   0.0161164609849135    0.983883539015087          B  \n28  0.00346528534198866    0.996534714658011          B  \n29    0.761253716426843    0.238746283573157          A  \n30   0.0597294571669348    0.940270542833065          B  \n31  0.00139900114299064    0.998600998857009          B  \n32   0.0146304515487079    0.985369548451292          B  \n33   0.0215114427320867    0.978488557267913          B  \n34  0.00359029891803412    0.996409701081966          B  \n35 8.92739861715428e-05    0.999910726013828          B  \n\n\nHow does this differ from the predictions from our first model?\n\n# Pull ldaf1 model predictions\nldaf_pred = predict(ldaf1)$class\n\n# Gather Predictions\nldaf_diff = data.frame(ldaf1 = as.character(ldaf_pred), ldaf2 = as.character(ldaf2$class))\n\n# Add match column\nldaf_diff$match = (ldaf_diff$ldaf1 == ldaf_diff$ldaf2)\n\n# Which ones are different?\nldaf_diff[which(ldaf_diff$match == F),]\n\n    ldaf1  ldaf2 match\n17    B      A   FALSE\n29    B      A   FALSE\n\n\nIndividuals 17 and 29 both differed in species prediction between the model fit and the jackknife posterior probability. Now let’s check the accuracy of our model fit:\n\n# Calculate error\nldaf_wrong = length(which(ldaf_pred != snake$Species)) # Number of incorrect predictions\nldaf_err = ldaf_wrong/nrow(snake) # Divide by number of individuals for error\n\n# Print error\nldaf_wrong\n\n[1] 5\n\nldaf_err\n\n[1] 0.1428571\n\n\nOur model classified 5 out of 35 (~14.3%) of the snakes as the incorrect species, meaning 30/35 were correct (~85.7%). Not bad, but can we do better?"
  },
  {
    "objectID": "a1b.html#model-selection",
    "href": "a1b.html#model-selection",
    "title": "2  Assignment 1b: Linear Discriminant Analysis",
    "section": "2.4 Model Selection",
    "text": "2.4 Model Selection\nOur previous model used all 6 measurements, but do we really need all of them, or are some of them unhelpful (or even detrimental)? To test this, we can run model selection using the stepclass() function:\n\n# stepclass package\nlibrary(klaR)\n\n# Model selection (forward)\nms_f = stepclass(Species ~ M1+M2+M3+M4+M5+M6,data=snake,\n          method=\"lda\", fold=35, direction=\"forward\")\n\n `stepwise classification', using 35-fold cross-validated correctness rate of method lda'.\n\n\n35 observations of 6 variables in 2 classes; direction: forward\n\n\nstop criterion: improvement less than 5%.\n\n\ncorrectness rate: 0.85714;  in: \"M6\";  variables (1): M6 \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.69 \n\n# Print model selection result\nms_f\n\nmethod      : lda \nfinal model : Species ~ M6\n&lt;environment: 0x0000014e93d579d0&gt;\n\ncorrectness rate = 0.8571 \n\n\nAfter model selection, we end up with a model using only M6 to predict species, with a correctness rate of 85.7%. This model has the same correctness as the full model, using only one measurement. In other words, this model is more efficient - it gets to the same accuracy using less information.\nThis model was generated using forward model selection, meaning the selection process works exclusively by adding variables to the model. We can also do the opposite:\n\n# stepclass package\nlibrary(klaR)\n\n# Model selection (forward)\nms_b = stepclass(Species ~ M1+M2+M3+M4+M5+M6,data=snake,\n          method=\"lda\", fold=35, direction=\"backward\")\n\n `stepwise classification', using 35-fold cross-validated correctness rate of method lda'.\n\n\n35 observations of 6 variables in 2 classes; direction: backward\n\n\nstop criterion: improvement less than 5%.\n\n\ncorrectness rate: 0.8;  starting variables (6): M1, M2, M3, M4, M5, M6 \ncorrectness rate: 0.85714;  out: \"M5\";  variables (5): M1, M2, M3, M4, M6 \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.86 \n\n# Print model selection result\nms_b\n\nmethod      : lda \nfinal model : Species ~ M1 + M2 + M3 + M4 + M6\n&lt;environment: 0x0000014e94300a70&gt;\n\ncorrectness rate = 0.8571 \n\n\nBackwards model selection works by removing variables from the full model. This means backwards selection should always return a model with a equal or more variables than forwards selection.\nLastly, we can run both:\n\n# stepclass package\nlibrary(klaR)\n\n# Model selection (forward)\nms_d = stepclass(Species ~ M1+M2+M3+M4+M5+M6,data=snake,\n          method=\"lda\", fold=35, direction=\"both\")\n\n `stepwise classification', using 35-fold cross-validated correctness rate of method lda'.\n\n\n35 observations of 6 variables in 2 classes; direction: both\n\n\nstop criterion: improvement less than 5%.\n\n\ncorrectness rate: 0.85714;  in: \"M6\";  variables (1): M6 \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.63 \n\n# Print model selection result\nms_d\n\nmethod      : lda \nfinal model : Species ~ M6\n&lt;environment: 0x0000014e94b4d588&gt;\n\ncorrectness rate = 0.8571"
  },
  {
    "objectID": "a1b.html#plotting-probabilities",
    "href": "a1b.html#plotting-probabilities",
    "title": "2  Assignment 1b: Linear Discriminant Analysis",
    "section": "2.5 Plotting Probabilities",
    "text": "2.5 Plotting Probabilities\nLets finish off by making some plots to visualize our LDA model results.\n\n# Pick a model to plot\nldaf3 = lda(Species ~ M6, data = snake)\n\n# Plot density curve\nplot(ldaf3, dimen = 1, type = 'dens')\n\n\n\n\nThis plots the posterior probabilites of an individual belonging to either species given its LD1 value. Remember from earlier that species A is associated with lower LD1 values.\nWe can also make this plot as a histogram:\n\n# Plot density curve\npar(mar = c (4,4,4,4))\nplot(ldaf3, dimen = 1, type = 'hist')\n\n\n\n\nOr combine both plots:\n\n# Plot density curve\npar(mar = c (4,4,4,4))\nplot(ldaf3, dimen = 1, type = 'both')\n\n\n\n\nAs always, we can also do this with ggplot too:\n\n# Predict species\nldaf3_pred = predict(ldaf3)\n\n# Plot\npred_species = as.data.frame(ldaf3_pred$x) # Gather LD1 values\npred_species$Species = snake$Species # Gather true species from data\n\n# Plot\nggplot(pred_species, aes(x = LD1, fill = Species))+\n  geom_density(alpha = 0.4)# alpha tells you how transparent the plots will be"
  },
  {
    "objectID": "a1b.html#tips-for-your-assignment",
    "href": "a1b.html#tips-for-your-assignment",
    "title": "2  Assignment 1b: Linear Discriminant Analysis",
    "section": "2.6 Tips for your assignment",
    "text": "2.6 Tips for your assignment\nSome things you may want to think about for your assignment:\n1. How would you pick which model you think is best? What factors would you consider? Are there any factors you would consider other than those discussed in this tutorial?\n2. LDA also assumes the data are independent. Do we know this assumption is respected? Why or why not? What would constitute it not being respected?\n3. How would you interpret your statistical results biologically (can be in terms of the snakes, how you would study them, or both)? You don’t have to be right, but don’t be vague, and don’t contradict your results."
  },
  {
    "objectID": "a1c.html#looking-at-the-data",
    "href": "a1c.html#looking-at-the-data",
    "title": "3  Assignment 1c: Cluster Analysis and Multidimensional Scaling",
    "section": "3.1 Looking at the data",
    "text": "3.1 Looking at the data\nYou know the drill by now:\n\n# Load in data\ndata = read.csv('monkey.csv', row.names = 1) # First column is row names\ndata # Print data\n\n      ind1 ind2 ind3 ind4 ind5 ind6 ind7 ind8 ind9 ind10 ind11 ind12 ind13\nind1    21    2    2   10    2    2    8    0    0     8    14    12     4\nind2     2   21   16    2   16    8    2    2    4     4     4     0     2\nind3     2   16   21    0   10   16    2    0    2     4     4     0     2\nind4    10    2    0   21    2    2   16    2    2     8    12     8     4\nind5     2   16   10    2   21   10    2    4    0     2     4     4     2\nind6     2    8   16    2   10   21    4    2    0     0     0     4     4\nind7     8    2    2   16    2    4   21    4    2    16     8     8     4\nind8     0    2    0    2    4    2    4   21    0     2     0     0     0\nind9     0    4    2    2    0    0    2    0   21     0     4     0     0\nind10    8    4    4    8    2    0   16    2    0    21    14    14     2\nind11   14    4    4   12    4    0    8    0    4    14    21    12     4\nind12   12    0    0    8    4    4    8    0    0    14    12    21     2\nind13    4    2    2    4    2    4    4    0    0     2     4     2    21\n\n\nOur data is a matrix containing the number of social interactions observed between individuals in a group of monkeys at the zoo. The matrix is symmetrical - the top/right half is identical to the bottom/left half."
  },
  {
    "objectID": "a1c.html#calculating-dissimilarity",
    "href": "a1c.html#calculating-dissimilarity",
    "title": "3  Assignment 1c: Cluster Analysis and Multidimensional Scaling",
    "section": "3.2 Calculating Dissimilarity",
    "text": "3.2 Calculating Dissimilarity\nFor this assignment we’ll be using 3 R functions: hclust, metaMDS (from the vegan package), isoMDS (from the MASSpackage), and cmdscale(). Let’s see what type of input data those functions need:\n\n# Check help functions\nlibrary(vegan)\nlibrary(MASS)\n?hclust()\n?metaMDS()\n?isoMDS()\n?cmdscale()\n\nYou’ll notice all of these functions require a dissimilarity matrix produced by dist. Let’s start by running dist().\n\n# Convert data to a dist object\ndist = as.dist(data)\ndist # Print dist\n\n      ind1 ind2 ind3 ind4 ind5 ind6 ind7 ind8 ind9 ind10 ind11 ind12\nind2     2                                                          \nind3     2   16                                                     \nind4    10    2    0                                                \nind5     2   16   10    2                                           \nind6     2    8   16    2   10                                      \nind7     8    2    2   16    2    4                                 \nind8     0    2    0    2    4    2    4                            \nind9     0    4    2    2    0    0    2    0                       \nind10    8    4    4    8    2    0   16    2    0                  \nind11   14    4    4   12    4    0    8    0    4    14            \nind12   12    0    0    8    4    4    8    0    0    14    12      \nind13    4    2    2    4    2    4    4    0    0     2     4     2\n\n\nNow our data is in a dist object. All of the redundant entries in the data have been removed.\nRight now, our data reflects similarity (i.e. high numbers reflect greater association between individuals). We need to convert it to dissimilarity. Dissimilarity is simply the opposite of similarity. We can convert similarity to dissimilarity by subtracting each data value from the maximum of the data.\n\n# Convert to dissimilarity\ndist = max(dist) - dist\ndist # Print dist\n\n      ind1 ind2 ind3 ind4 ind5 ind6 ind7 ind8 ind9 ind10 ind11 ind12\nind2    14                                                          \nind3    14    0                                                     \nind4     6   14   16                                                \nind5    14    0    6   14                                           \nind6    14    8    0   14    6                                      \nind7     8   14   14    0   14   12                                 \nind8    16   14   16   14   12   14   12                            \nind9    16   12   14   14   16   16   14   16                       \nind10    8   12   12    8   14   16    0   14   16                  \nind11    2   12   12    4   12   16    8   16   12     2            \nind12    4   16   16    8   12   12    8   16   16     2     4      \nind13   12   14   14   12   14   12   12   16   16    14    12    14\n\n\nNow we’re ready to run our analyses!"
  },
  {
    "objectID": "a1c.html#hierarchical-cluster-analysis",
    "href": "a1c.html#hierarchical-cluster-analysis",
    "title": "3  Assignment 1c: Cluster Analysis and Multidimensional Scaling",
    "section": "3.3 Hierarchical Cluster Analysis",
    "text": "3.3 Hierarchical Cluster Analysis\nRemember from lecture there are 4 types of hierarchical cluster analysis:\n\nSingle linkage\nAverage linkage\nComplete linkage\nWard linkage\n\nLet’s run through them one by one:\n\n3.3.1 Single linkage\nWe can run all 4 types of cluster analysis using the hclust() R function:\n\n# run single linkage cluster analysis\nclust_1 = hclust(dist, method = 'single')\nclust_1 # print object\n\n\nCall:\nhclust(d = dist, method = \"single\")\n\nCluster method   : single \nNumber of objects: 13 \n\n\nPrinting the hclust object doesn’t really tell us much. For more detail, we’re going to have to plot it:\n\n# Plot single linkage tree\nplot(clust_1, hang = -1, main = 'Single linkage', \n     ylab = 'Dissimilarity', # Label y axis\n     xlab = '', sub = '') # Remove x-axis label\n\n\n\n\nThis outputs a tree showing the associations between our individual monkeys. dissimilarity is on the y-axis. The greater the distance between individuals on the y-axis, the greater their dissimilarity. Our tree has grouped the monkeys according to how frequently they interact with each other. For example. individuals 2, 3, 5, and 6 interact often, as evidenced by their low dissimilarity.\nBut how well does this tree fit the data? To answer that question, we need to calculate the cophenetic correlation coefficient (CCC):\n\n# Calculate CCC\ncoph_1 = cophenetic(clust_1) # Get cophenetic\nccc_1 = cor(coph_1, dist) # Calculate correlation of the cophenetic with the data\nccc_1 # Print CCC\n\n[1] 0.9036043\n\n\nThat’s a pretty high correlation coefficient, indicating our dendrogram represented the structure in the original data very well. Let’s try some other methods:\n\n\n3.3.2 Average Linkage\n\n# run cluster analysis\nclust_2 = hclust(dist, method = 'average')\n\n# Plot\nplot(clust_2, hang = -1, main = 'Average linkage', ylab = 'Dissimilarity', xlab = '', sub = '')\n\n\n\n# Calculate CCC\ncoph_2 = cophenetic(clust_2)\nccc_2 = cor(coph_2, dist)\nccc_2\n\n[1] 0.9288949\n\n\n\n\n3.3.3 Complete Linkage\n\n# run cluster analysis\nclust_3 = hclust(dist, method = 'complete')\n\n# Plot\nplot(clust_3, hang = -1, main = 'Complete linkage', ylab = 'Dissimilarity', xlab = '', sub = '')\n\n\n\n# Calculate CCC\ncoph_3 = cophenetic(clust_3)\nccc_3 = cor(coph_3, dist)\nccc_3\n\n[1] 0.9141956\n\n\n\n\n3.3.4 Ward Linkage\n\n# run cluster analysis\nclust_4 = hclust(dist, method = 'ward.D')\n\n# Plot\nplot(clust_4, hang = -1, main = 'Ward linkage', ylab = 'Dissimilarity', xlab = '', sub = '')\n\n\n\n# Calculate CCC\ncoph_4 = cophenetic(clust_4)\nccc_4 = cor(coph_4, dist)\nccc_4\n\n[1] 0.7633159\n\n\nEach method gives a slightly different tree and CCC value. Where are they similar? Where do they differ? Which one(s) would you trust? Why?"
  },
  {
    "objectID": "a1c.html#multidimensional-scaling",
    "href": "a1c.html#multidimensional-scaling",
    "title": "3  Assignment 1c: Cluster Analysis and Multidimensional Scaling",
    "section": "3.4 Multidimensional Scaling",
    "text": "3.4 Multidimensional Scaling\nAnother method we can use to test for associations between our monkeys is multidimensional scaling (MDS). There are two types of MDS: non-metric, and metric MDS. Let’s start with non-metric MDS.\n\n3.4.1 Non-Metric MDS\n\n# Run non-metric MDS - metaMDS\nmds1 = metaMDS(dist, wascores = F)\n\nRun 0 stress 0.07592385 \nRun 1 stress 0.07358653 \n... New best solution\n... Procrustes: rmse 0.1926081  max resid 0.5628466 \nRun 2 stress 0.07575137 \nRun 3 stress 0.08568213 \nRun 4 stress 0.07239301 \n... New best solution\n... Procrustes: rmse 0.1712365  max resid 0.5176449 \nRun 5 stress 0.07882366 \nRun 6 stress 0.07875788 \nRun 7 stress 0.08055022 \nRun 8 stress 0.07227846 \n... New best solution\n... Procrustes: rmse 0.01249288  max resid 0.03088889 \nRun 9 stress 0.1734347 \nRun 10 stress 0.07358653 \nRun 11 stress 0.07875788 \nRun 12 stress 0.07875788 \nRun 13 stress 0.072393 \n... Procrustes: rmse 0.0124638  max resid 0.03095045 \nRun 14 stress 0.07875788 \nRun 15 stress 0.07575138 \nRun 16 stress 0.07366297 \nRun 17 stress 0.1998376 \nRun 18 stress 0.07358653 \nRun 19 stress 0.120356 \nRun 20 stress 0.07875788 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n    13: stress ratio &gt; sratmax\n     7: scale factor of the gradient &lt; sfgrmin\n\n# Print mds results\nmds1\n\n\nCall:\nmetaMDS(comm = dist, wascores = F) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     dist \nDistance: user supplied \n\nDimensions: 2 \nStress:     0.07227846 \nStress type 1, weak ties\nBest solution was not repeated after 20 tries\nThe best solution was from try 8 (random start)\nScaling: centring, PC rotation \nSpecies: scores missing\n\n\nBy default, metaMDS has two dimensions. This MDS has a stress value of 0.072. Remember from lecture that stress &lt; 0.10 is a “good representation”, so this MDS result is pretty good. If we want, we can test different numbers of dimensions (k) and create a scree plot to find the best one:\n\n# Create a container object\nscree = data.frame(k = 1:5, stress = NA)\n\n# Loop through k 1 to 5\nfor(k in 1:5){\n  \n  # Run MDS\n  mds = metaMDS(dist, wascores = F, k = k) # Set k to our loop index\n  \n  # Pull out stress\n  scree[k,'stress'] = mds$stress # Fill kth row of the column 'stress' in scree\n  \n} # End loop\n\n\n# Print results\nscree\n\n  k       stress\n1 1 2.712906e-01\n2 2 7.239303e-02\n3 3 8.814077e-05\n4 4 3.763269e-04\n5 5 9.500430e-05\n\n# Make scree plot\nplot(stress ~ k, data = scree, # Plot stress against k\n     type = 'b', # Lines and points\n     pch = 16) # Point 16 (filled circle)\nabline(h = 0.1, lty = 'dashed') # Plot a dashed line at 0.1\n\n\n\n\nWe have an elbow at k=3, but we also get warnings that our dataset may be too small using k=3. The stress at k=2 is low enough that we can stick to using that.\nLet’s plot our results:\n\n# Plot result\nplot(mds1, type = 't')\n\nspecies scores not available\n\n\n\n\n\nHere we’ve plotted the values of our two MDS dimensions against each other for each individual. Similar to the cluster analysis, we see certain individuals are grouped together. Is it the same groups of individuals? What does that tell you about your results?\nLet’s try a different non-metric MDS function:\n\n# Run non-metric MDS - isoMDS\nmds2 = isoMDS(dist)\n\nError in isoMDS(dist): zero or negative distance between objects 2 and 3\n\n\nUh oh. This function doesn’t like zeroes in the data. Let’s fix that by translating our data to proportions, and adding a small increment.\n\n# Translate to proportions\ndist2 = dist/max(dist)\n\n# Add an increment\ndist2 = dist2 + 0.0001\n\n# Print new dist\ndist2\n\n        ind1   ind2   ind3   ind4   ind5   ind6   ind7   ind8   ind9  ind10\nind2  0.8751                                                               \nind3  0.8751 0.0001                                                        \nind4  0.3751 0.8751 1.0001                                                 \nind5  0.8751 0.0001 0.3751 0.8751                                          \nind6  0.8751 0.5001 0.0001 0.8751 0.3751                                   \nind7  0.5001 0.8751 0.8751 0.0001 0.8751 0.7501                            \nind8  1.0001 0.8751 1.0001 0.8751 0.7501 0.8751 0.7501                     \nind9  1.0001 0.7501 0.8751 0.8751 1.0001 1.0001 0.8751 1.0001              \nind10 0.5001 0.7501 0.7501 0.5001 0.8751 1.0001 0.0001 0.8751 1.0001       \nind11 0.1251 0.7501 0.7501 0.2501 0.7501 1.0001 0.5001 1.0001 0.7501 0.1251\nind12 0.2501 1.0001 1.0001 0.5001 0.7501 0.7501 0.5001 1.0001 1.0001 0.1251\nind13 0.7501 0.8751 0.8751 0.7501 0.8751 0.7501 0.7501 1.0001 1.0001 0.8751\n       ind11  ind12\nind2               \nind3               \nind4               \nind5               \nind6               \nind7               \nind8               \nind9               \nind10              \nind11              \nind12 0.2501       \nind13 0.7501 0.8751\n\n\nLet’s make sure this doesn’t mess with our results:\n\n# Run non-metric MDS - metaMDS\nmds1 = metaMDS(dist2, wascores = F)\n\nRun 0 stress 0.07575137 \nRun 1 stress 0.07358653 \n... New best solution\n... Procrustes: rmse 0.1923138  max resid 0.5629988 \nRun 2 stress 0.1172799 \nRun 3 stress 0.07308015 \n... New best solution\n... Procrustes: rmse 0.1725227  max resid 0.5167782 \nRun 4 stress 0.1832293 \nRun 5 stress 0.0757299 \nRun 6 stress 0.07227846 \n... New best solution\n... Procrustes: rmse 0.01388241  max resid 0.04162179 \nRun 7 stress 0.08571329 \nRun 8 stress 0.07572991 \nRun 9 stress 0.07227846 \n... New best solution\n... Procrustes: rmse 2.027274e-06  max resid 3.417192e-06 \n... Similar to previous best\nRun 10 stress 0.07366297 \nRun 11 stress 0.07366297 \nRun 12 stress 0.0757299 \nRun 13 stress 0.0757299 \nRun 14 stress 0.08055013 \nRun 15 stress 0.07239301 \n... Procrustes: rmse 0.01245452  max resid 0.03091433 \nRun 16 stress 0.07366297 \nRun 17 stress 0.07227846 \n... New best solution\n... Procrustes: rmse 2.642746e-06  max resid 4.669585e-06 \n... Similar to previous best\nRun 18 stress 0.08569496 \nRun 19 stress 0.07358653 \nRun 20 stress 0.08055013 \n*** Best solution repeated 1 times\n\n# Print mds results\nmds1\n\n\nCall:\nmetaMDS(comm = dist2, wascores = F) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     dist2 \nDistance: user supplied \n\nDimensions: 2 \nStress:     0.07227846 \nStress type 1, weak ties\nBest solution was repeated 1 time in 20 tries\nThe best solution was from try 17 (random start)\nScaling: centring, PC rotation \nSpecies: scores missing\n\n# Plot result\nplot(mds1, type = 't')\n\nspecies scores not available\n\n\n\n\n\nThe values have shifted around a bit but the structure and interpretation of the plot is the same. Let’s continue on:\n\n# Run non-metric MDS - isoMDS\nmds2 = isoMDS(dist2)\n\ninitial  value 24.760322 \niter   5 value 14.153502\niter  10 value 12.254154\niter  15 value 11.639473\niter  20 value 11.360460\nfinal  value 11.341572 \nconverged\n\n# Print output\nmds2\n\n$points\n            [,1]         [,2]\nind1   0.5060798  0.103253718\nind2  -0.6157097 -0.285522725\nind3  -0.6133549 -0.310223762\nind4   0.5008471  0.144443835\nind5  -0.6264450 -0.279477605\nind6  -0.6228134 -0.325577873\nind7   0.4940123  0.147103149\nind8  -0.6783876  0.680257989\nind9  -0.2575043  1.075686777\nind10  0.5482152  0.033286470\nind11  0.5478840  0.082682890\nind12  0.5895070  0.001005794\nind13  0.2276697 -1.066918657\n\n$stress\n[1] 11.34157\n\n\nThe modelling algorithms seems to be a little different, and we end up with a different stress result - in this case, one that is above the 10% threshold (note that stress is in % in this function, unlike metaMDS where it is in proportion). Let’s try another scree plot:\n\n# Create a container object\nscree = data.frame(k = 1:5, stress = NA)\n\n# Loop through k 1 to 5\nfor(k in 1:5){\n  \n  # Run MDS\n  mds = isoMDS(dist2, k = k) # Set k to our loop index\n  \n  # Pull out stress\n  scree[k,'stress'] = mds$stress # Fill kth row of the column 'stress' in scree\n  \n} # End loop\n\n\n# Print results\nscree\n\n  k      stress\n1 1 36.54857293\n2 2 11.34157190\n3 3  0.04614441\n4 4  0.12630298\n5 5  0.19439990\n\n# Make scree plot\nplot(stress ~ k, data = scree, # Plot stress against k\n     type = 'b', # Lines and points\n     pch = 16) # Point 16 (filled circle)\nabline(h = 10, lty = 'dashed') # Plot a dashed line at 0.1\n\n\n\n\nIn this case, it seems we’re better off using 3 dimensions:\n\n# Run non-metric MDS - isoMDS\nmds2 = isoMDS(dist2, k = 3)\n\ninitial  value 18.960422 \niter   5 value 11.725940\niter  10 value 6.417141\niter  15 value 4.149185\niter  20 value 1.466748\niter  25 value 0.764657\niter  30 value 0.449114\niter  35 value 0.302911\niter  40 value 0.156116\niter  45 value 0.087536\niter  50 value 0.046144\nfinal  value 0.046144 \nstopped after 50 iterations\n\n# Print output\nmds2\n\n$points\n            [,1]       [,2]       [,3]\nind1   2.0028765  0.4079841 -0.4202376\nind2  -2.0901193 -1.3657278  1.2720375\nind3  -2.0910832 -1.3666943  1.2729356\nind4   2.0066696  0.4042391 -0.4161698\nind5  -2.0896284 -1.3631915  1.2769256\nind6  -2.0921681 -1.3637290  1.2724192\nind7   2.0032691  0.4111298 -0.4185160\nind8  -2.1600174  2.2033842 -1.8938700\nind9  -0.6641520  2.9972846  2.5707229\nind10  2.0009095  0.4046552 -0.4185896\nind11  2.0041166  0.4044773 -0.4197885\nind12  2.0019583  0.4023552 -0.4223419\nind13 -0.8326313 -2.1761669 -3.2555274\n\n$stress\n[1] 0.04614441\n\n\nLet’s plot our results:\n\n# Plot isoMDS\nplot(mds2$points[,1], mds2$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS') # Labelling\n\n# Plot individual names\ntext(mds2$points[,1], mds2$points[,2], rownames(data))\n\n\n\n\nAll of our grouped individuals are plotted on top of each other. Let’s try adding some random jiggle so we can see them\n\n# Plot isoMDS\nplot(mds2$points[,1], mds2$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS', # Labelling\n     xlim = c(-2.5, 2.5), ylim = c(3, -3)) # Set axis limits\n\n# Set random seed for consistency\nset.seed(1212)\n\n# Plot individual names\ntext(mds2$points[,1] + rnorm(13, 0, 0.2), # Add random values pulled from a \n     mds2$points[,2] + rnorm(13, 0, 0.2), # normal distribution with mean 0, sd 0.2\n     rownames(data)) # Add names\n\n\n\n\nThat’s a bit better. We can also add some color to this plot if we want - say, individuals 6 to 9 are juveniles:\n\n# Plot isoMDS\nplot(mds2$points[,1], mds2$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS', # Labelling\n     xlim = c(-2.5, 2.5), ylim = c(3, -3)) # Set axis limits\n\n# Set random seed for consistency\nset.seed(1212)\n\n# Juvenile identifier\nad = c(rep(1,5), rep(0,4), rep(1,4)) # ad is 1 for first 5 and last 4\nad\n\n [1] 1 1 1 1 1 0 0 0 0 1 1 1 1\n\n# Plot individual names\ntext(mds2$points[,1] + rnorm(13, 0, 0.2), # Add random values pulled from a \n     mds2$points[,2] + rnorm(13, 0, 0.2), # normal distribution with mean 0, sd 0.2\n     rownames(data), # Add names\n     col = ifelse(ad == 0, 'purple', 'orange')) # color \n# Add a legend\nlegend('topright', legend = c('Juvenile', 'Adult'), fill = c('purple', 'orange'))\n\n\n\n\nDoes this plot match the previous one, and/or the cluster analyses?\n\n\n3.4.2 Metric MDS\nWe can run metric MDS using the cmdscale() function:\n\n# run metric MDS\nmds3 = cmdscale(dist, eig = T)\nmds3\n\n$points\n             [,1]       [,2]\nind1   5.84977914  2.7981654\nind2  -7.46899061 -0.4452479\nind3  -7.87360160  2.4742095\nind4   6.04970828 -1.1964346\nind5  -6.77887791  2.8518073\nind6  -7.21161499  3.9150940\nind7   4.79289905 -0.9896933\nind8  -2.46317365 -5.3304368\nind9  -1.86216019 -9.7770302\nind10  5.45394235  1.1317599\nind11  5.27120921 -0.1097836\nind12  6.20052885  3.6063451\nind13  0.04035206  1.0712450\n\n$eig\n [1]  4.150450e+02  1.794715e+02  1.684147e+02  1.309366e+02  6.931537e+01\n [6]  5.811388e+01  3.306204e+01  1.780496e+01 -4.263256e-14 -8.643935e+00\n[11] -2.208342e+01 -4.468321e+01 -7.952271e+01\n\n$x\nNULL\n\n$ac\n[1] 0\n\n$GOF\n[1] 0.4844901 0.5545014\n\n\nMetric MDS doesn’t have stress. Instead, we have to look at goodness of fit (GOF) to assess how well the analysis worked. GOF is similar to an R2 value, where numbers closer to 1 indicate a better fit (though be wary of overfitting!). There are two different GOF values for each metric MDS.\nAs with the other MDS functions, k defaults to 2. We can make another scree plot:\n\n# Create a container object\nscree = data.frame(k = 1:5, GOF1 = NA, GOF2 = NA)\n\n# Loop through k 1 to 5\nfor(k in 1:5){\n  \n  # Run MDS\n  mds = cmdscale(dist, eig = T, k = k) # Set k to our loop index\n  \n  # Pull out stress\n  scree[k,c(2,3)] = mds$GOF # Fill kth row of the GOF columns in scree\n  \n} # End loop\n\n\n# Print results\nscree\n\n  k      GOF1      GOF2\n1 1 0.3382331 0.3871096\n2 2 0.4844901 0.5545014\n3 3 0.6217365 0.7115806\n4 4 0.7284408 0.8337043\n5 5 0.7849281 0.8983543\n\n# Make scree plot\nplot(GOF2 ~ k, data = scree, # Plot stress against k\n     type = 'b', # Lines and points\n     pch = 16, # Point 16 (filled circle)\n     ylab = 'Goodness of Fit', ylim = c(0.3, 1))\npoints(GOF1 ~ k, data = scree, type = 'b', pch = 16, col = 'red') # Add second GOF value\nabline(h = 0.1, lty = 'dashed') # Plot a dashed line at 0.1\nlegend('topleft', pch = 16, legend = c('GOF1', 'GOF2'), col = c('red', 'black')) # Add legend\n\n\n\n\nGoodness of fit scales linearly, so what k to use is more of a judgement call.\n\n# run metric MDS\nmds3 = cmdscale(dist, k=4, eig = T)\nmds3\n\n$points\n             [,1]       [,2]       [,3]       [,4]\nind1   5.84977914  2.7981654  1.2685787 -0.6738375\nind2  -7.46899061 -0.4452479  2.6496404  2.1091477\nind3  -7.87360160  2.4742095  3.4326421  1.2339175\nind4   6.04970828 -1.1964346 -0.9306802 -1.2319775\nind5  -6.77887791  2.8518073 -1.2465315  2.3989342\nind6  -7.21161499  3.9150940 -1.9699687 -2.4369605\nind7   4.79289905 -0.9896933 -2.7067201 -0.2043847\nind8  -2.46317365 -5.3304368 -9.4034890  2.0126521\nind9  -1.86216019 -9.7770302  5.2905629 -1.1532342\nind10  5.45394235  1.1317599  0.3565324  4.1657272\nind11  5.27120921 -0.1097836  4.1678975  1.4238728\nind12  6.20052885  3.6063451 -0.2995254  1.5298300\nind13  0.04035206  1.0712450 -0.6089393 -9.1736869\n\n$eig\n [1]  4.150450e+02  1.794715e+02  1.684147e+02  1.309366e+02  6.931537e+01\n [6]  5.811388e+01  3.306204e+01  1.780496e+01 -4.263256e-14 -8.643935e+00\n[11] -2.208342e+01 -4.468321e+01 -7.952271e+01\n\n$x\nNULL\n\n$ac\n[1] 0\n\n$GOF\n[1] 0.7284408 0.8337043\n\n\nLet’s plot the first two dimensions:\n\n# Plot metric MDS\nplot(mds3$points[,1], mds3$points[,2], # MDS dimension 1 and 2 values\n     type = 'n', # Don't plot any points\n     xlab = 'Dim 1', ylab = 'Dim 2', main = 'Metric MDS') # Labelling\n\n# Plot individual names\ntext(mds3$points[,1], # Add random values pulled from a \n     mds3$points[,2], # normal distribution with mean 0, sd 0.2\n     rownames(data), # Add names\n     col = ifelse(ad == 0, 'purple', 'orange')) # color \n# Add a legend\nlegend('bottomright', legend = c('Juvenile', 'Adult'), fill = c('purple', 'orange'))\n\n\n\n\n\n\n3.4.3 3D Plotting (Optional)\nIt may not be necessary, but if your MDS has more than 2 dimensions, you can try plotting it in three dimensions and see if it helps:\n\nlibrary(plot3D)\n\n# Prepare data to plot\nx = mds3$points[,1]\ny = mds3$points[,2]\nz = mds3$points[,3]\n\n# Create 3D plot\nscatter3D(x,y,z, colvar = NULL, col = 'blue', \n          pch = 16, cex = 0.5, bty = 'g', theta = 5)\n\n# Add text\ntext3D(x, \n       # Add some jiggle to the labels\n       y+rnorm(13, mean = 0, sd = 0.5), z + rnorm(13, mean = 0, sd = 0.5), \n                    labels = names(mds3$points[,1]), add = T, colkey = F, \n                    cex = 0.5, adj = 1, d = 2)"
  },
  {
    "objectID": "a1c.html#mantel-test-graduate-students-only",
    "href": "a1c.html#mantel-test-graduate-students-only",
    "title": "3  Assignment 1c: Cluster Analysis and Multidimensional Scaling",
    "section": "3.5 Mantel Test (Graduate Students Only)",
    "text": "3.5 Mantel Test (Graduate Students Only)\nWe can infer to some extent whether juveniles and adults preferentially associate with each other from our colored MDS plots, but we can also test it statistically using a Mantel test. To run the Mantel test, we need to convert our adult index into a dist object:\n\n# Create dist matrix for adults\nad_dist = dist(ad)\nad_dist\n\n   1 2 3 4 5 6 7 8 9 10 11 12\n2  0                         \n3  0 0                       \n4  0 0 0                     \n5  0 0 0 0                   \n6  1 1 1 1 1                 \n7  1 1 1 1 1 0               \n8  1 1 1 1 1 0 0             \n9  1 1 1 1 1 0 0 0           \n10 0 0 0 0 0 1 1 1 1         \n11 0 0 0 0 0 1 1 1 1  0      \n12 0 0 0 0 0 1 1 1 1  0  0   \n13 0 0 0 0 0 1 1 1 1  0  0  0\n\n\nNote this is dissimilarity: adult-juvenile pairs are assigned 1, and same-class pairs are assigned 0.\nThe Mantel test looks for correlation between this matrix and our original dissociation matrix, and statistically tests if the associations are different from what we would expect due to chance.\n\n# Run mantel test\nlibrary(ade4)\nmantel.rtest(ad_dist, dist, nrepet = 999)\n\nWarning in is.euclid(m1): Zero distance(s)\n\n\nWarning in is.euclid(m2): Zero distance(s)\n\n\nMonte-Carlo test\nCall: mantelnoneuclid(m1 = m1, m2 = m2, nrepet = nrepet)\n\nObservation: 0.1686576 \n\nBased on 999 replicates\nSimulated p-value: 0.073 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n 1.369210491 -0.001062026  0.015364686 \n\n\nIt’s very close, but we don’t have statistically significant evidence that juveniles and adults associate preferentially with each other in this case."
  },
  {
    "objectID": "a1c.html#tips-for-your-assignment",
    "href": "a1c.html#tips-for-your-assignment",
    "title": "3  Assignment 1c: Cluster Analysis and Multidimensional Scaling",
    "section": "3.6 Tips for your Assignment:",
    "text": "3.6 Tips for your Assignment:\nSome things you may want to think about for your assignment:\n1. How would you pick which cluster analyses and MDS analyses are best for your data? Are they conceptual, or do they have to do with the results? Do they agree?\n2. How would you interpret your statistical results biologically? You don’t have to be right, but don’t be vague, and don’t contradict your results."
  },
  {
    "objectID": "a1d.html",
    "href": "a1d.html",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "",
    "text": "5"
  },
  {
    "objectID": "a1d.html#looking-at-the-data",
    "href": "a1d.html#looking-at-the-data",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "4.1 Looking at the data",
    "text": "4.1 Looking at the data\n\n# Read in data\ndata = read.csv('Schoenemann.csv')\n\n# View data structure\nhead(data)\n\n      Order     Family   Genus    Species Location   Mass    Fat   FFWT    CNS\n1 Carnivora    Felidae   Felis canadensis   Alaska 7688.0 1120.0 6568.0 105.09\n2 Carnivora    Felidae   Felis      rufus Virginia 6152.0  738.0 5414.0  81.75\n3 Carnivora Mustelidae    Gulo     luscus   Alaska 9362.0  562.0 8800.0  85.36\n4 Carnivora Mustelidae Mustela    erminea   Alaska  183.3    3.1  180.2   6.69\n5 Carnivora Mustelidae Mustela      vison Virginia 1032.0   66.0  966.0  18.06\n6 Carnivora Proyonidae Procyon      lotor Virginia 6040.0 1013.0 5027.0  58.31\n  HEART  MUSCLE   BONE\n1 27.59 4341.45 631.18\n2 25.45 3600.31 552.23\n3 80.96 5271.20 879.12\n4  1.87  104.70  21.98\n5  7.63  581.53  80.27\n6 36.19 2920.69 517.78\n\ndim(data)\n\n[1] 39 12\n\n\nThe Schoenemann dataset contains 39 observations of 12 variables, describing to the morphometry of different species of mammals, along with metadata describing them. Let’s start by getting rid of the metadata. We won’t need it for this assignment.\n\n# Remove metadata\ndata = data[,which(colnames(data) == 'Mass'):ncol(data)] # I do it this way to avoid hard coding\n\n# check if it worked\nhead(data)\n\n    Mass    Fat   FFWT    CNS HEART  MUSCLE   BONE\n1 7688.0 1120.0 6568.0 105.09 27.59 4341.45 631.18\n2 6152.0  738.0 5414.0  81.75 25.45 3600.31 552.23\n3 9362.0  562.0 8800.0  85.36 80.96 5271.20 879.12\n4  183.3    3.1  180.2   6.69  1.87  104.70  21.98\n5 1032.0   66.0  966.0  18.06  7.63  581.53  80.27\n6 6040.0 1013.0 5027.0  58.31 36.19 2920.69 517.78\n\n\nNow we only have numeric data left. Let’s take a look at the data graphically.\n\n# Looking at the data\nboxplot(data)\n\n\n\n# Loop through columns to create histograms\nfor(i in 1:ncol(data)){hist(data[,i], main = colnames(data)[i])} # Name histogram according to column name"
  },
  {
    "objectID": "a1d.html#considering-transformations",
    "href": "a1d.html#considering-transformations",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "4.2 Considering Transformations",
    "text": "4.2 Considering Transformations\nWe can see that all these data are exponentially distributed - there is much more data at small values, and the data at higher values is spread out. If we run our regressions on this data, our assumptions are going to be violated:\n\n# Run a test model and check assumptions\ntest = lm(CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, data = data)\n\n# Check for normality as an example\n\n# Residual histogram\nhist(residuals(test), 20)\n\n\n\n# QQplot\nqqnorm(residuals(test))\n\n\n\n# Statistical test for normality\nshapiro.test(residuals(test))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(test)\nW = 0.87473, p-value = 0.0004494\n\n\nThose diagnostics look… less than ideal.\nThis is a textbook case of when to apply a log transformation - remember logging is the opposite of exponentiating:\n\n# Apply log transformation\ndata_l = log(data)\n\n# Check out the new data\nhead(data_l)\n\n      Mass      Fat     FFWT      CNS     HEART   MUSCLE     BONE\n1 8.947416 7.021084 8.789965 4.654817 3.3174534 8.375964 6.447591\n2 8.724533 6.603944 8.596743 4.403666 3.2367157 8.188775 6.313965\n3 9.144414 6.331502 9.082507 4.446878 4.3939552 8.570013 6.778921\n4 5.211124 1.131402 5.194067 1.900614 0.6259384 4.651099 3.090133\n5 6.939254 4.189655 6.873164 2.893700 2.0320878 6.365663 4.385396\n6 8.706159 6.920672 8.522579 4.065774 3.5887828 7.979575 6.249550\n\n# Looking at the data\nboxplot(data_l)\n\n\n\n# Loop through columns to create histograms\nfor(i in 1:ncol(data_l)){hist(data_l[,i], main = colnames(data_l)[i])} # Name histogram according to column name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow our data looks much more uniform.\nAlways remember that transforming your data incorrectly or unnecessarily can do more harm than good. How do you decide if it is helpful to transform your data? What is the purpose of transforming your data? Think carefully about these questions for your assignment when you’re deciding whether to transform the data for your assignment."
  },
  {
    "objectID": "a1d.html#simple-linear-regression",
    "href": "a1d.html#simple-linear-regression",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "4.3 Simple Linear Regression",
    "text": "4.3 Simple Linear Regression\nNow that our data is good to go, we’re going to run some simple linear regressions to predict central nervous system mass (CNS). Simple linear regressions only have one predictor variable. Linear regression is run using the lm() command:\n\n# Run simple linear regressions - Mass\nm1 = lm(CNS ~ Mass, data = data_l) # run model\nsummary(m1) # model summary\n\n\nCall:\nlm(formula = CNS ~ Mass, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77785 -0.20227 -0.05439  0.19607  0.78453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.79097    0.14844  -18.80   &lt;2e-16 ***\nMass         0.77105    0.02556   30.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3657 on 37 degrees of freedom\nMultiple R-squared:  0.9609,    Adjusted R-squared:  0.9599 \nF-statistic: 909.7 on 1 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Run simple linear regressions - Fat\nm2 = lm(CNS ~ Fat, data = data_l) # run model\nsummary(m2) # model summary\n\n\nCall:\nlm(formula = CNS ~ Fat, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.22918 -0.41510  0.01431  0.36008  1.38000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.15712    0.13223  -1.188    0.242    \nFat          0.59903    0.03518  17.028   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6223 on 37 degrees of freedom\nMultiple R-squared:  0.8868,    Adjusted R-squared:  0.8838 \nF-statistic: 289.9 on 1 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Run simple linear regressions - FFWT\nm3 = lm(CNS ~ FFWT, data = data_l) # run model\nsummary(m3) # model summary\n\n\nCall:\nlm(formula = CNS ~ FFWT, data = data_l)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8057 -0.2112 -0.0535  0.1907  0.7654 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.80193    0.14382  -19.48   &lt;2e-16 ***\nFFWT         0.78494    0.02515   31.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3539 on 37 degrees of freedom\nMultiple R-squared:  0.9634,    Adjusted R-squared:  0.9624 \nF-statistic: 973.7 on 1 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Run simple linear regressions - HEART\nm4 = lm(CNS ~ HEART, data = data_l) # run model\nsummary(m4) # model summary\n\n\nCall:\nlm(formula = CNS ~ HEART, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75646 -0.16000 -0.03248  0.15018  0.85234 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.99514    0.05853   17.00   &lt;2e-16 ***\nHEART        0.88201    0.02872   30.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3594 on 37 degrees of freedom\nMultiple R-squared:  0.9622,    Adjusted R-squared:  0.9612 \nF-statistic:   943 on 1 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Run simple linear regressions - MUSCLE\nm5 = lm(CNS ~ MUSCLE, data = data_l) # run model\nsummary(m5) # model summary\n\n\nCall:\nlm(formula = CNS ~ MUSCLE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82059 -0.15588 -0.00489  0.17331  0.80475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.20579    0.11856  -18.61   &lt;2e-16 ***\nMUSCLE       0.76488    0.02296   33.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3323 on 37 degrees of freedom\nMultiple R-squared:  0.9677,    Adjusted R-squared:  0.9669 \nF-statistic:  1109 on 1 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Run simple linear regressions - BONE\nm6 = lm(CNS ~ BONE, data = data_l) # run model\nsummary(m6) # model summary\n\n\nCall:\nlm(formula = CNS ~ BONE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.10309 -0.24611  0.01155  0.25195  0.63931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -1.0497     0.1042  -10.07 3.75e-12 ***\nBONE          0.7856     0.0277   28.36  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3879 on 37 degrees of freedom\nMultiple R-squared:  0.956, Adjusted R-squared:  0.9548 \nF-statistic: 804.4 on 1 and 37 DF,  p-value: &lt; 2.2e-16\n\n\nIn this case, it looks like all of our variables are strong, significant predictors with high R2 values.\nLet’s plot all of these regressions:\n\n# Plot regressions\n\n# Plot simple linear regressions - Mass\nplot(CNS ~ Mass, data = data_l, pch = 16) # plot points\nabline(m1, lwd = 2, col = 'red') # Plot model\n\n\n\n# Plot simple linear regressions - Fat\nplot(CNS ~ Fat, data = data_l, pch = 16) # plot points\nabline(m2, lwd = 2, col = 'red') # Plot model\n\n\n\n# Plot simple linear regressions - FFWT\nplot(CNS ~ FFWT, data = data_l, pch = 16) # plot points\nabline(m3, lwd = 2, col = 'red') # Plot model\n\n\n\n# Plot simple linear regressions - HEART\nplot(CNS ~ HEART, data = data_l, pch = 16) # plot points\nabline(m4, lwd = 2, col = 'red') # Plot model\n\n\n\n# Plot simple linear regressions - MUSCLE\nplot(CNS ~ MUSCLE, data = data_l, pch = 16) # plot points\nabline(m5, lwd = 2, col = 'red') # Plot model\n\n\n\n# Plot simple linear regressions - BONE\nplot(CNS ~ BONE, data = data_l, pch = 16) # plot points\nabline(m6, lwd = 2, col = 'red') # Plot model\n\n\n\n\nAll of the regression slopes are positive. This makes sense - larger animals tend to have larger brains."
  },
  {
    "objectID": "a1d.html#multiple-linear-regression",
    "href": "a1d.html#multiple-linear-regression",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "4.4 Multiple Linear Regression",
    "text": "4.4 Multiple Linear Regression\nWe’ve made 6 models using 1 variable. Now, let’s try making 1 model with 6 variables:\n\n# Run full model\nm7 = lm(CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, data = data_l)\nsummary(m7)\n\n\nCall:\nlm(formula = CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, \n    data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.72690 -0.12073  0.00376  0.08672  0.85638 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.61138    1.18496  -0.516   0.6094  \nMass        -0.46867    2.14250  -0.219   0.8282  \nFat         -0.06818    0.15489  -0.440   0.6628  \nFFWT        -0.02606    2.30347  -0.011   0.9910  \nHEART        0.41894    0.21913   1.912   0.0649 .\nMUSCLE       1.03524    0.61123   1.694   0.1000  \nBONE        -0.06339    0.32054  -0.198   0.8445  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3286 on 32 degrees of freedom\nMultiple R-squared:  0.9727,    Adjusted R-squared:  0.9676 \nF-statistic: 190.1 on 6 and 32 DF,  p-value: &lt; 2.2e-16\n\n\nOur full model has a very(!) high R2 value, and contrary to the simple linear regressions where every predictor was significant, none of our predictors are considered significant in the final model at \\(\\alpha\\) = 0.05. Why do you think that is?"
  },
  {
    "objectID": "a1d.html#checking-assumptions",
    "href": "a1d.html#checking-assumptions",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "4.5 Checking Assumptions",
    "text": "4.5 Checking Assumptions\nNow that we’ve run our full model, it’s time to check its assumptions. Those assumptions are Independence, Linearity, Homoscedasticity, and Normality. By now, you should be familiar with what these all mean, but let’s run through them anyways:\n\n4.5.1 Independence\nThe assumption of independence states that the value of each data point (‘datum’, if you will) is independent of all other data points. Some of the ways in which it could be violated may not be testable (e.g. if they have to do with how the data was collected), but what we can test for is autocorrelation. Autocorrelation translates to self correlation (auto = self). We can test for autocorrelation statistically using a Durbin-Watson test, and visually using an autocorrelation function on the residuals:\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# Durbin-watson test\ndwtest(m7)\n\n\n    Durbin-Watson test\n\ndata:  m7\nDW = 2.4315, p-value = 0.8545\nalternative hypothesis: true autocorrelation is greater than 0\n\n# Autocorrelation function\nacf(residuals(m7))\n\n\n\n\nThe Durbin-Watson test returns an insignificant p-value, indicating no autocorrelation structure is present. The ACF plots the correlation coefficient of the data against itself using lags. Lag 0 correlates the data against itself, which is always 1. Lag 1 correlates each data point against the point after it, and so on. All of the correlation coefficients are between the blue lines, so again, we have no autocorrelation structure, and we can say independence is respected.\n\n\n4.5.2 Linearity\nThe assumption of linearity states that the response variable consistently scales linearly with its predictors. We can test for linearity statistically using Ramsey’s RESET test on our model:\n\n# Run RESET test\nresettest(m7)\n\n\n    RESET test\n\ndata:  m7\nRESET = 0.12784, df1 = 2, df2 = 30, p-value = 0.8805\n\n\nIn this case, the p-value is not significant, meaning the assumption of linearity is respected.\n\n\n4.5.3 Homoscedasticity\nThe assumption of homoscedasticity is that the variance in the data is independent of the value of the data - i.e. the variance in the data is consistent. We can test this statistically using the Breusch-Pagan test, and visually by plotting the model residuals against the fitted values.\n\n# Run Breusch-Pagan test\nbptest(m7)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m7\nBP = 15.974, df = 6, p-value = 0.01389\n\n# Plot residuals vs fitted\nplot(m7$residuals ~ m7$fitted.values, pch = 16); abline(h = 0)\n\n\n\n# Can also be done using plot.lm, ?plot.lm for details\nplot(m7, 1)\n\n\n\n\nThe Breusch-Pagan returns a significant p-value, indicating the assumption of homoscedasticity is violated. We can see in the residuals versus fitted plot that the variance in the data is smaller at low values than it is at higher values (the points on the left of the plot are clustered more closely than they are on the right). Let’s come back to this later.\n\n\n4.5.4 Normality\nThe assumption of normality states that the residuals of our model should be normally distributed. If they aren’t, that would indicate that our model is biased towards overprediction or underprediction in some way. As we did earlier in the transformation section, we can check for normality visually by looking at histograms and QQ plots of our residuals, and statistically by running a Shapiro-Wilk test on the residuals.\n\n# Residual histogram\nhist(residuals(m7))\n\n\n\n# QQplot\nqqnorm(residuals(m7))\n\n\n\n# Can also use plot.lm for qqplot\nplot(m7, 2)\n\n\n\n# Statistical test for normality\nshapiro.test(residuals(m7))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m7)\nW = 0.95391, p-value = 0.1113\n\n\nThe Shapiro-Wilk test p-value is not significant (though it treads close), meaning the assumption of normality is respected. The residual histogram largely looks normal, and the QQ plot tails start to pull off the line at high and low values, possibly indicating outliers are causing us some trouble, but not enough to violate the assumption.\n\n\n4.5.5 What if my assumptions aren’t respected?\nThe typical fixes for violated assumptions are data transformations, and the removal of outliers. In our case, we pass all assumptions except for homoscedasticity. We’ve already transformed our data to meet the assumption of normality, so further transformation is likely off the table, though we could potentially try different transformations. We could also try outlier removal - our model diagnostics using plot.lm() identify three outliers - 19, 20, and 28. Feel free to play around with removing outliers if you want.\nKeep in mind that data transformations and removing outliers both represent trade-offs. Removing outliers may help meet your model assumptions, but you may also be removing data that reflects reality from your model. In that case, is it really helping you to remove outliers? Similarly, transforming your data may help you meet your assumptions, but in a case like this, transforming our data further or in a different way could end up violating other assumptions. Sometimes the best way to deal with violated assumptions is simply to state that they are violated and think about what that means for the interpretation of your model. Play around with all these different ideas, and come up with what you think is best. At the end of the day, a lot of statistical choices are judgement calls, with no perfect right answer."
  },
  {
    "objectID": "a1d.html#model-selection",
    "href": "a1d.html#model-selection",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "4.6 Model Selection",
    "text": "4.6 Model Selection\nIn assignment 1b, we created 1 model with 6 variables, then tested if we could get a similarly effective mode using fewer variables - i.e. a more efficient model. Let’s do the same thing here:\n\n# Stepwise model selection - forward\nm8 = step(m7, direction = 'forward')\n\nStart:  AIC=-80.52\nCNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE\n\nsummary(m8)\n\n\nCall:\nlm(formula = CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, \n    data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.72690 -0.12073  0.00376  0.08672  0.85638 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.61138    1.18496  -0.516   0.6094  \nMass        -0.46867    2.14250  -0.219   0.8282  \nFat         -0.06818    0.15489  -0.440   0.6628  \nFFWT        -0.02606    2.30347  -0.011   0.9910  \nHEART        0.41894    0.21913   1.912   0.0649 .\nMUSCLE       1.03524    0.61123   1.694   0.1000  \nBONE        -0.06339    0.32054  -0.198   0.8445  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3286 on 32 degrees of freedom\nMultiple R-squared:  0.9727,    Adjusted R-squared:  0.9676 \nF-statistic: 190.1 on 6 and 32 DF,  p-value: &lt; 2.2e-16\n\n\nRunning forward model selection cuts the model down to 3 variables. As with 1b, we can also do backward:\n\n# Stepwise model selection - backwards\nm9 = step(m7, direction = 'backward')\n\nStart:  AIC=-80.52\nCNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- FFWT    1   0.00001 3.4552 -82.523\n- BONE    1   0.00422 3.4594 -82.476\n- Mass    1   0.00517 3.4604 -82.465\n- Fat     1   0.02092 3.4761 -82.288\n&lt;none&gt;                3.4552 -80.523\n- MUSCLE  1   0.30975 3.7650 -79.175\n- HEART   1   0.39468 3.8499 -78.305\n\nStep:  AIC=-82.52\nCNS ~ Mass + Fat + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- BONE    1   0.00487 3.4601 -84.468\n- Fat     1   0.04499 3.5002 -84.019\n- Mass    1   0.05110 3.5063 -83.951\n&lt;none&gt;                3.4552 -82.523\n- MUSCLE  1   0.38148 3.8367 -80.439\n- HEART   1   0.39621 3.8514 -80.289\n\nStep:  AIC=-84.47\nCNS ~ Mass + Fat + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n- Fat     1   0.04057 3.5007 -86.014\n- Mass    1   0.09476 3.5549 -85.415\n&lt;none&gt;                3.4601 -84.468\n- HEART   1   0.40303 3.8631 -82.171\n- MUSCLE  1   0.41063 3.8707 -82.095\n\nStep:  AIC=-86.01\nCNS ~ Mass + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                3.5007 -86.014\n- Mass    1   0.35155 3.8522 -84.282\n- HEART   1   0.37281 3.8735 -84.067\n- MUSCLE  1   0.85479 4.3555 -79.493\n\nsummary(m9)\n\n\nCall:\nlm(formula = CNS ~ Mass + HEART + MUSCLE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74267 -0.11882 -0.00818  0.10790  0.84702 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.1840     0.8704  -0.211  0.83383   \nMass         -0.8197     0.4372  -1.875  0.06919 . \nHEART         0.3855     0.1997   1.931  0.06166 . \nMUSCLE        1.2436     0.4254   2.923  0.00603 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3163 on 35 degrees of freedom\nMultiple R-squared:  0.9723,    Adjusted R-squared:   0.97 \nF-statistic: 410.2 on 3 and 35 DF,  p-value: &lt; 2.2e-16\n\n\nAnd both:\n\n# Stepwise model selection - both\nm10 = step(m7, direction = 'both')\n\nStart:  AIC=-80.52\nCNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- FFWT    1   0.00001 3.4552 -82.523\n- BONE    1   0.00422 3.4594 -82.476\n- Mass    1   0.00517 3.4604 -82.465\n- Fat     1   0.02092 3.4761 -82.288\n&lt;none&gt;                3.4552 -80.523\n- MUSCLE  1   0.30975 3.7650 -79.175\n- HEART   1   0.39468 3.8499 -78.305\n\nStep:  AIC=-82.52\nCNS ~ Mass + Fat + HEART + MUSCLE + BONE\n\n         Df Sum of Sq    RSS     AIC\n- BONE    1   0.00487 3.4601 -84.468\n- Fat     1   0.04499 3.5002 -84.019\n- Mass    1   0.05110 3.5063 -83.951\n&lt;none&gt;                3.4552 -82.523\n+ FFWT    1   0.00001 3.4552 -80.523\n- MUSCLE  1   0.38148 3.8367 -80.439\n- HEART   1   0.39621 3.8514 -80.289\n\nStep:  AIC=-84.47\nCNS ~ Mass + Fat + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n- Fat     1   0.04057 3.5007 -86.014\n- Mass    1   0.09476 3.5549 -85.415\n&lt;none&gt;                3.4601 -84.468\n+ BONE    1   0.00487 3.4552 -82.523\n+ FFWT    1   0.00067 3.4594 -82.476\n- HEART   1   0.40303 3.8631 -82.171\n- MUSCLE  1   0.41063 3.8707 -82.095\n\nStep:  AIC=-86.01\nCNS ~ Mass + HEART + MUSCLE\n\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                3.5007 -86.014\n+ Fat     1   0.04057 3.4601 -84.468\n- Mass    1   0.35155 3.8522 -84.282\n+ FFWT    1   0.01839 3.4823 -84.219\n- HEART   1   0.37281 3.8735 -84.067\n+ BONE    1   0.00046 3.5002 -84.019\n- MUSCLE  1   0.85479 4.3555 -79.493\n\nsummary(m10)\n\n\nCall:\nlm(formula = CNS ~ Mass + HEART + MUSCLE, data = data_l)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74267 -0.11882 -0.00818  0.10790  0.84702 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.1840     0.8704  -0.211  0.83383   \nMass         -0.8197     0.4372  -1.875  0.06919 . \nHEART         0.3855     0.1997   1.931  0.06166 . \nMUSCLE        1.2436     0.4254   2.923  0.00603 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3163 on 35 degrees of freedom\nMultiple R-squared:  0.9723,    Adjusted R-squared:   0.97 \nF-statistic: 410.2 on 3 and 35 DF,  p-value: &lt; 2.2e-16\n\n\nIf you want to get fancy, we can even look at every possible model\n\nlibrary(MuMIn)\n\n# Set global options to avoid error\noptions('na.action' = na.fail)\n\n# Run dredge to get full selection table\ndredge(m7, rank = 'AIC')\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: lm(formula = CNS ~ Mass + Fat + FFWT + HEART + MUSCLE + BONE, \n    data = data_l)\n---\nModel selection table \n   (Intrc)     BONE       Fat     FFWT  HEART    Mass  MUSCL df  logLik   AIC\n57 -0.1840                             0.3855 -0.8197 1.2440  5  -8.332  26.7\n43 -1.1710          -0.124700          0.3924         0.5758  5  -8.631  27.3\n45 -0.2475                    -0.85860 0.3612         1.2890  5  -8.961  27.9\n59 -0.4891          -0.061790          0.4060 -0.5708 1.0530  6  -8.104  28.2\n47 -0.5634          -0.096360 -0.57650 0.4140         1.0840  6  -8.124  28.2\n41 -1.1430                             0.2965         0.5107  4 -10.198  28.4\n61 -0.3049                     0.58870 0.3869 -1.2410 1.0870  6  -8.229  28.5\n49 -1.7220                                    -0.6191 1.3760  4 -10.305  28.6\n58 -0.2132 -0.01926                    0.3888 -0.8022 1.2420  6  -8.329  28.7\n33 -2.2060                                            0.7649  3 -11.347  28.7\n44 -1.2470 -0.17370 -0.114900          0.4320         0.6975  6  -8.363  28.7\n35 -2.4620          -0.085950                         0.8664  4 -10.603  29.2\n42 -1.2550 -0.24820                    0.3639         0.6919  5  -9.670  29.3\n37 -1.7080                    -0.64360                1.3900  4 -10.670  29.3\n46 -0.3146 -0.03584           -0.80970 0.3672         1.2710  6  -8.953  29.9\n60 -0.6121 -0.06454 -0.066880          0.4188 -0.4915 1.0320  7  -8.077  30.2\n63 -0.4991          -0.070960 -0.17140 0.4086 -0.4111 1.0710  7  -8.101  30.2\n48 -0.6664 -0.05386 -0.097010 -0.50110 0.4234         1.0550  7  -8.106  30.2\n15 -1.3260          -0.141700  0.53690 0.4770                 5 -10.136  30.3\n62 -0.4490 -0.07620            0.72770 0.4004 -1.2710 1.0430  7  -8.195  30.4\n29 -0.9446                     2.32300 0.4383 -1.8930         5 -10.222  30.4\n53 -1.8390                     0.54460        -1.0080 1.2310  5 -10.226  30.5\n50 -1.4810  0.10920                           -0.7284 1.3790  5 -10.226  30.5\n34 -2.3590 -0.10740                                   0.8684  4 -11.244  30.5\n51 -1.9150          -0.030810                 -0.4897 1.2850  5 -10.253  30.5\n27 -1.3680          -0.174800          0.4968  0.5506         5 -10.291  30.6\n39 -2.0530          -0.062690 -0.43960                1.2660  5 -10.335  30.7\n36 -2.5030 -0.03440 -0.083230                         0.8963  5 -10.593  31.2\n38 -1.4490  0.10160           -0.79230                1.4370  5 -10.612  31.2\n16 -1.6470 -0.20880 -0.139600  0.71490 0.5072                 6  -9.877  31.8\n30 -1.3160 -0.23860            2.54100 0.4741 -1.9060         6  -9.885  31.8\n13 -1.0500                     0.42240 0.4110                 4 -11.885  31.8\n31 -1.1730          -0.087570  1.35400 0.4642 -0.8572         6 -10.045  32.1\n64 -0.6114 -0.06339 -0.068180 -0.02606 0.4189 -0.4687 1.0350  8  -8.077  32.2\n28 -1.6410 -0.16770 -0.182100          0.5254  0.6983         6 -10.119  32.2\n54 -1.6320  0.07991            0.40060        -0.9854 1.2720  6 -10.189  32.4\n52 -1.6650  0.09609 -0.024700                 -0.6115 1.3050  6 -10.194  32.4\n55 -1.8600          -0.005303  0.48770        -0.9454 1.2310  6 -10.225  32.5\n40 -1.7890  0.10350 -0.062910 -0.59040                1.3130  6 -10.273  32.5\n21 -2.8120                     2.53800        -1.7250         4 -12.557  33.1\n25 -0.7492                             0.4796  0.3550         4 -12.592  33.2\n14 -1.4090 -0.23040            0.62070 0.4454                 5 -11.597  33.2\n32 -1.4890 -0.22300 -0.075840  1.68700 0.4941 -1.0080         7  -9.751  33.5\n5  -2.8020                     0.78490                        3 -13.802  33.6\n7  -3.2380          -0.110900  0.92010                        4 -12.820  33.6\n56 -1.6680  0.08319 -0.011140  0.27490        -0.8521 1.2720  7 -10.186  34.4\n19 -3.4510          -0.166800                  0.9721         4 -13.253  34.5\n9   0.9951                             0.8820                 3 -14.404  34.8\n12  0.3485  0.33210 -0.111800          0.6667                 5 -12.415  34.8\n10  0.3284  0.25560                    0.5985                 4 -13.429  34.9\n22 -3.0100 -0.08987            2.62700        -1.7240         5 -12.513  35.0\n23 -2.8680          -0.014310  2.38200        -1.5540         5 -12.553  35.1\n26 -0.8465 -0.06583                    0.4905  0.4097         5 -12.568  35.1\n6  -3.0010 -0.09063            0.87490                        4 -13.760  35.5\n8  -3.3620 -0.05836 -0.109700  0.97660                        5 -12.802  35.6\n11  1.1350          -0.071490          0.9798                 4 -13.981  36.0\n17 -2.7910                                     0.7710         3 -15.079  36.2\n20 -3.4240  0.01165 -0.166400                  0.9601         5 -13.252  36.5\n24 -3.0350 -0.08761 -0.007846  2.53900        -1.6310         6 -12.511  37.0\n18 -2.5850  0.09433                            0.6790         4 -15.033  38.1\n2  -1.0500  0.78560                                           3 -17.378  40.8\n4  -1.1140  0.84970 -0.052420                                 4 -17.190  42.4\n3  -0.1571           0.599000                                 3 -35.811  77.6\n1   1.3230                                                    2 -78.299 160.6\n    delta weight\n57   0.00  0.104\n43   0.60  0.077\n45   1.26  0.056\n59   1.55  0.048\n47   1.58  0.047\n41   1.73  0.044\n61   1.79  0.042\n49   1.95  0.039\n58   1.99  0.038\n33   2.03  0.038\n44   2.06  0.037\n35   2.54  0.029\n42   2.68  0.027\n37   2.68  0.027\n46   3.24  0.021\n60   3.49  0.018\n63   3.54  0.018\n48   3.55  0.018\n15   3.61  0.017\n62   3.73  0.016\n29   3.78  0.016\n53   3.79  0.016\n50   3.79  0.016\n34   3.82  0.015\n51   3.84  0.015\n27   3.92  0.015\n39   4.01  0.014\n36   4.52  0.011\n38   4.56  0.011\n16   5.09  0.008\n30   5.11  0.008\n13   5.11  0.008\n31   5.43  0.007\n64   5.49  0.007\n28   5.57  0.006\n54   5.71  0.006\n52   5.72  0.006\n55   5.79  0.006\n40   5.88  0.005\n21   6.45  0.004\n25   6.52  0.004\n14   6.53  0.004\n32   6.84  0.003\n5    6.94  0.003\n7    6.98  0.003\n56   7.71  0.002\n19   7.84  0.002\n9    8.14  0.002\n12   8.17  0.002\n10   8.19  0.002\n22   8.36  0.002\n23   8.44  0.002\n26   8.47  0.002\n6    8.86  0.001\n8    8.94  0.001\n11   9.30  0.001\n17   9.49  0.001\n20   9.84  0.001\n24  10.36  0.001\n18  11.40  0.000\n2   14.09  0.000\n4   15.72  0.000\n3   50.96  0.000\n1  133.93  0.000\nModels ranked by AIC(x) \n\n\nHere, we’re ranking models by AIC. AIC balances fit with model complexity. Lower values of AIC are considered better. Generally, 2 is used as a rule of thumb for AIC - if delta AIC is &gt;2, the lower AIC model is considered better. If delat AIC is &lt;2, the support for the best model is weak, and the models could even be considered “tied”."
  },
  {
    "objectID": "a1d.html#tips-for-your-assignment",
    "href": "a1d.html#tips-for-your-assignment",
    "title": "4  Assignment 1d: Multiple Linear Regression",
    "section": "5.1 Tips for your Assignment:",
    "text": "5.1 Tips for your Assignment:\nSome things you may want to think about for your assignment:\n\nWhat role is collinearity playing in your assignment? Is it something you should be concerned about? Why or why not?\nWhat does it mean if your assumptions are violated? How would you fix it? Is it worth fixing it? Why or why not?\nHow would you interpret your statistical results biologically? You don’t have to be right, but don’t be vague, and don’t contradict your results."
  },
  {
    "objectID": "a1e.html#looking-at-the-data",
    "href": "a1e.html#looking-at-the-data",
    "title": "5  Assignment 1e: Bayesian Data Analysis",
    "section": "5.1 Looking at the Data",
    "text": "5.1 Looking at the Data\n\n# Load in data\ndata = read.csv('cuse.csv')\n\n# Look at the data structure\nhead(data)\n\n  X   age education wantsMore notUsing using\n1 1   &lt;25       low       yes       53     6\n2 2   &lt;25       low        no       10     4\n3 3   &lt;25      high       yes      212    52\n4 4   &lt;25      high        no       50    10\n5 5 25-29       low       yes       60    14\n6 6 25-29       low        no       19    10\n\ndim(data)\n\n[1] 16  6\n\n\nOur data contains 16 observations of 5 variables - a binomial matrix of how many women are using or not using birth control within 16 groups, and three categorical predictors - age, expressed as a bin, education, and whether they want more children. The first column is a duplicate of our row names. We can get rid of that:\n\n# Remove column 1\ndata = data[,-1]\nhead(data)\n\n    age education wantsMore notUsing using\n1   &lt;25       low       yes       53     6\n2   &lt;25       low        no       10     4\n3   &lt;25      high       yes      212    52\n4   &lt;25      high        no       50    10\n5 25-29       low       yes       60    14\n6 25-29       low        no       19    10"
  },
  {
    "objectID": "a1e.html#binomial-glm",
    "href": "a1e.html#binomial-glm",
    "title": "5  Assignment 1e: Bayesian Data Analysis",
    "section": "5.2 Binomial GLM",
    "text": "5.2 Binomial GLM\nBinomial general linear models are used to calculate the probability of a binomial response - in this case, whether someone is using or not using birth control. Binomial GLM response can be fed in either as a true/false set, or as a matrix of successes and failures. According to ?family, we need to feed in the data with successes first and failures second. Let’s create the matrix:\n\n# create response matrix\nresp = cbind(data$using, data$notUsing)\nhead(resp)\n\n     [,1] [,2]\n[1,]    6   53\n[2,]    4   10\n[3,]   52  212\n[4,]   10   50\n[5,]   14   60\n[6,]   10   19\n\n\nIn this case, all of our variables are categorical, and they are currently stored as characters:\n\n# Check predictor classes\nclass(data$age)\n\n[1] \"character\"\n\nclass(data$education)\n\n[1] \"character\"\n\nclass(data$wantsMore)\n\n[1] \"character\"\n\n\nThese should function fine as categorical variables. Let’s make our GLM:\n\n# Run GLM\nm1 = glm(resp ~ age + education + wantsMore, family = 'binomial', data = data)\nsummary(m1) # Summary\n\n\nCall:\nglm(formula = resp ~ age + education + wantsMore, family = \"binomial\", \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***\nage25-29       0.3894     0.1759   2.214  0.02681 *  \nage30-39       0.9086     0.1646   5.519 3.40e-08 ***\nage40-49       1.1892     0.2144   5.546 2.92e-08 ***\neducationlow  -0.3250     0.1240  -2.620  0.00879 ** \nwantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 165.772  on 15  degrees of freedom\nResidual deviance:  29.917  on 10  degrees of freedom\nAIC: 113.43\n\nNumber of Fisher Scoring iterations: 4\n\n\nIn our summary we see we have 5 predictors: The age bin, low education, and wanting more kids. High education and not wanting more kids are missing because these variables are binary, so we only need one variable to differentiate them. We can also see the values of our model coefficients, their standard errors, and the model AIC."
  },
  {
    "objectID": "a1e.html#making-it-bayesian",
    "href": "a1e.html#making-it-bayesian",
    "title": "5  Assignment 1e: Bayesian Data Analysis",
    "section": "5.3 Making it Bayesian",
    "text": "5.3 Making it Bayesian\nThe default GLM function is frequentist (that’s why we have p-values). Now lets try a Bayesian approach:\n\n# Stan\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.32.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(shinystan)\n\nLoading required package: shiny\n\n\n\nThis is shinystan version 2.6.0\n\nlibrary(ggplot2)\n\n# Run glm\nm2 = stan_glm(resp ~ age + education + wantsMore, family = 'binomial', data = data)\n\n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000118 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.18 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.153 seconds (Warm-up)\nChain 1:                0.151 seconds (Sampling)\nChain 1:                0.304 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.7e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.147 seconds (Warm-up)\nChain 2:                0.156 seconds (Sampling)\nChain 2:                0.303 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.158 seconds (Warm-up)\nChain 3:                0.145 seconds (Sampling)\nChain 3:                0.303 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.151 seconds (Warm-up)\nChain 4:                0.157 seconds (Sampling)\nChain 4:                0.308 seconds (Total)\nChain 4: \n\nsummary(m2) # Summary\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      resp ~ age + education + wantsMore\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   6\n\nEstimates:\n               mean   sd   10%   50%   90%\n(Intercept)  -0.8    0.2 -1.0  -0.8  -0.6 \nage25-29      0.4    0.2  0.2   0.4   0.6 \nage30-39      0.9    0.2  0.7   0.9   1.1 \nage40-49      1.2    0.2  0.9   1.2   1.5 \neducationlow -0.3    0.1 -0.5  -0.3  -0.2 \nwantsMoreyes -0.8    0.1 -1.0  -0.8  -0.7 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 31.7    1.6 29.6  31.7  33.8 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2127 \nage25-29      0.0  1.0  2358 \nage30-39      0.0  1.0  2176 \nage40-49      0.0  1.0  2086 \neducationlow  0.0  1.0  3176 \nwantsMoreyes  0.0  1.0  3310 \nmean_PPD      0.0  1.0  4075 \nlog-posterior 0.0  1.0  1993 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nThe stan_glm function automatically feeds our model into Stan, which is a Hamiltonian Markov Chain Monte Carlo (MCMC) sampler. Running summary on our model gives us some model diagnostics - all our Rhat values are 1 and all our n_eff values are well into the thousands, both of which are a good sign. We can also do some visual checks and tests:\n\n# Trace plot\nplot(m2, 'trace')\n\n\n\n\nThese are trace plots, which show us the parameter values selected for each iteration of the MCMC chain. We want these to look “fuzzy” - that indicates the sampler is exploring the full range of possible values. If these lines were to be flat, that would indicate the sampler got “stuck” and didn’t sample the full posterior distributions. These look good.\nLets look at our posteriors:\n\n# Plot parameter values with uncertainties\nplot(m2, prob_outer = 0.95)\n\n\n\n# Plot posterior distributions\nplot(m2, 'mcmc_hist')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThese plots both give us an idea of our parameter values and their posterior distributions. The former plot shows the median parameter estimates (circle), their 50% quantiles (dark blue box), and their 95% quantiles (thin blue line). The latter shows histograms of the posterior distributions of each of our parameters.\nWe can also pull out our coefficients and posteriors directly\n\n# Model coefficients\nm2$coefficients\n\n (Intercept)     age25-29     age30-39     age40-49 educationlow wantsMoreyes \n  -0.8090834    0.3881481    0.9127747    1.1921744   -0.3264688   -0.8373483 \n\n# Model posteriors\nposterior &lt;- as.matrix(m2)\n\n# Plot model posteriors (95% quantile)\nplot_title &lt;- ggtitle(\"Posterior distributions with medians and 95% credible intervals\")\n\nmcmc_areas(posterior, pars = names(m2$coefficients),\n           prob = 0.95) + plot_title\n\n\n\n\nHow would you interpret these plots?"
  },
  {
    "objectID": "a1e.html#adding-priors",
    "href": "a1e.html#adding-priors",
    "title": "5  Assignment 1e: Bayesian Data Analysis",
    "section": "5.4 Adding Priors",
    "text": "5.4 Adding Priors\nLets try adding some priors:\n\n# Run glm with priors\nm3 = stan_glm(resp ~ age + education + wantsMore, family = 'binomial', data = data,\n              prior = normal(location = c(0.2, 1.5, 2, -1, -0.25), # Normal priors, means\n                             scale = c(0.03, 0.03, 0.03, 0.03, 0.03))) # And standard deviations\n\n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.243 seconds (Warm-up)\nChain 1:                0.113 seconds (Sampling)\nChain 1:                0.356 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.7e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.224 seconds (Warm-up)\nChain 2:                0.106 seconds (Sampling)\nChain 2:                0.33 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.201 seconds (Warm-up)\nChain 3:                0.114 seconds (Sampling)\nChain 3:                0.315 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'binomial' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.8e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.214 seconds (Warm-up)\nChain 4:                0.129 seconds (Sampling)\nChain 4:                0.343 seconds (Total)\nChain 4: \n\nsummary(m3) # Summary\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      resp ~ age + education + wantsMore\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   6\n\nEstimates:\n               mean   sd   10%   50%   90%\n(Intercept)  -1.2    0.1 -1.3  -1.2  -1.1 \nage25-29      0.2    0.0  0.2   0.2   0.3 \nage30-39      1.5    0.0  1.4   1.5   1.5 \nage40-49      2.0    0.0  2.0   2.0   2.0 \neducationlow -1.0    0.0 -1.0  -1.0  -0.9 \nwantsMoreyes -0.3    0.0 -0.3  -0.3  -0.2 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 31.7    1.5 29.8  31.8  33.8 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  6014 \nage25-29      0.0  1.0  6066 \nage30-39      0.0  1.0  6159 \nage40-49      0.0  1.0  6568 \neducationlow  0.0  1.0  6899 \nwantsMoreyes  0.0  1.0  6404 \nmean_PPD      0.0  1.0  4811 \nlog-posterior 0.0  1.0  1690 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nLets look at our plots again:\n\n# Trace plot\nplot(m3, 'trace')\n\n\n\n# Plot parameter values with uncertainties\nplot(m3, prob_outer = 0.95)\n\n\n\n# Plot posterior distributions\nplot(m3, 'mcmc_hist')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n# Model coefficients\nm3$coefficients\n\n (Intercept)     age25-29     age30-39     age40-49 educationlow wantsMoreyes \n  -1.1980341    0.2201313    1.4813526    1.9956072   -0.9704883   -0.2822287 \n\n# Model posteriors\nposterior &lt;- as.matrix(m3)\n\n# Plot model posteriors (95% quantile)\nplot_title &lt;- ggtitle(\"Posterior distributions with medians and 95% credible intervals\")\n\nmcmc_areas(posterior, pars = names(m3$coefficients),\n           prob = 0.95) + plot_title\n\n\n\n\nYou can also look at all of your Stan model results using shinystan by running launch_shinystan(model). Try it out on your end (it doesn’t work in markdown)"
  },
  {
    "objectID": "a1e.html#tips-for-your-assignment",
    "href": "a1e.html#tips-for-your-assignment",
    "title": "5  Assignment 1e: Bayesian Data Analysis",
    "section": "5.5 Tips for your Assignment:",
    "text": "5.5 Tips for your Assignment:\nSome things you may want to think about for your assignment:\n\nHow do the results of these three models differ? Why or why not?\nDo you believe certain models are more or less correct? Why or why not?\nHow would you interpret your statistical results biologically? You don’t have to be right, but don’t be vague, and don’t contradict your results."
  }
]